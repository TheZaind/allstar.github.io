{
    "questions": [
        {
            "question": "ğŸŒ³ Ein XML-Dokument muss immer eine Baumstruktur haben. Was ist das charakteristische Merkmal dieser Struktur?",
            "options": [
                "Alle Elemente mÃ¼ssen auf derselben Hierarchieebene stehen",
                "Es gibt genau ein Wurzel-Element, das alle anderen Elemente umschlieÃŸt",
                "Jedes Element kann beliebig viele Ã¼bergeordnete Elemente haben",
                "Die Struktur muss mindestens 3 Ebenen tief sein"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸŒ³ Die XML-Baumstruktur basiert auf einem einzigen Wurzel-Element (Root-Element), das alle anderen Elemente hierarchisch umschlieÃŸt. Dies ist ein fundamentales Prinzip von XML - nach der XML-Deklaration folgt immer genau ein Wurzel-Element, in dem alle weiteren Elemente verschachtelt sind.\n\n**Warum andere Optionen falsch sind:**\n- Option A: Elemente stehen in verschiedenen Hierarchieebenen (Parent-Child-Beziehungen)\n- Option C: Jedes Element hat maximal ein Ã¼bergeordnetes Element (eindeutige Hierarchie)\n- Option D: Die Tiefe ist nicht vorgeschrieben - auch flache Strukturen sind gÃ¼ltig ğŸ“Š",
            "difficulty": "einsteiger",
            "category": "XML-Grundlagen"
        },
        {
            "question": "ğŸ“ Bei der Erstellung von XML-Dateien fÃ¼r einen internationalen Datenaustausch mÃ¼ssen Dateinamen-Richtlinien beachtet werden. Welcher Dateiname entspricht den Best Practices?",
            "options": [
                "MÃ¼ller&SÃ¶hne_Produktkatalog_2025.xml",
                "produktkatalog_mÃ¼ller_sÃ¶hne_2025.xml",
                "PRODUKTKATALOG_mueller_soehne_2025.xml",
                "produktkatalog_mueller_soehne_2025.xml"
            ],
            "correct": 3,
            "explain": "**BegrÃ¼ndung:** ğŸ“ Option D befolgt alle wichtigen Richtlinien fÃ¼r XML-Dateinamen:\n- âœ… Keine Umlaute (mÃ¼ller â†’ mueller, sÃ¶hne â†’ soehne)\n- âœ… Keine Sonderzeichen (&-Zeichen vermieden)\n- âœ… Konsistente Kleinschreibung\n- âœ… Underscore als Trennzeichen\n\n**Probleme der anderen Optionen:**\n- Option A: Sonderzeichen (&) und Umlaute (Ã¶)\n- Option B: Umlaute (Ã¼, Ã¶) kÃ¶nnen KompatibilitÃ¤tsprobleme verursachen\n- Option C: Inkonsistente GroÃŸ-/Kleinschreibung (PRODUKTKATALOG vs. mueller) ğŸš«\n\n**Praxisrelevanz:** Diese Regeln gewÃ¤hrleisten KompatibilitÃ¤t zwischen verschiedenen Betriebssystemen und Servern! ğŸŒ",
            "difficulty": "einsteiger",
            "category": "XML-Best-Practices"
        },
        {
            "question": "ğŸ” Analysieren Sie diese DTD-Zeile: <!ELEMENT song (title, year, duration?)>. Was bedeutet diese Definition fÃ¼r die XML-Struktur?",
            "options": [
                "Ein song-Element muss title und year enthalten, duration ist optional",
                "Ein song-Element kann beliebig viele title, year und duration Elemente haben",
                "Ein song-Element muss entweder title, year oder duration enthalten",
                "Ein song-Element kann title und year mehrfach enthalten, duration nur einmal"
            ],
            "correct": 0,
            "explain": "**BegrÃ¼ndung:** ğŸ” Die DTD-Syntax verwendet spezielle Zeichen fÃ¼r HÃ¤ufigkeitsangaben:\n- **title, year** (ohne Zeichen): Genau einmal erforderlich âœ…\n- **duration?** (Fragezeichen): Optional - kann 0 oder 1 mal vorkommen â“\n\n**DTD-Syntax-Reminder:**\n- `+` = Ein- oder mehrmals\n- `*` = Kein-, ein- oder mehrmals  \n- `?` = Kein- oder einmal (optional)\n- `|` = Oder-VerknÃ¼pfung\n- `()` = Gruppierung\n\n**Praktische Anwendung:** Diese Definition ist typisch fÃ¼r Musikdatenbanken, wo Titel und Jahr Pflichtfelder sind, die Spieldauer aber optional erfasst wird ğŸµ",
            "difficulty": "fortgeschritten",
            "category": "DTD-Validierung"
        },
        {
            "question": "âš–ï¸ Was ist der Unterschied zwischen 'wohlgeformtem' und 'validem' XML?",
            "options": [
                "Wohlgeformt bedeutet korrekte Syntax, valid bedeutet zusÃ¤tzlich DTD/Schema-KonformitÃ¤t",
                "Wohlgeformt und valid sind synonyme Begriffe",
                "Valid bedeutet korrekte Syntax, wohlgeformt bedeutet zusÃ¤tzlich DTD-KonformitÃ¤t",
                "Wohlgeformt gilt nur fÃ¼r HTML, valid nur fÃ¼r XML"
            ],
            "correct": 0,
            "explain": "**BegrÃ¼ndung:** âš–ï¸ Dies sind zwei verschiedene QualitÃ¤tsstufen in XML:\n\n**Wohlgeformt (Well-formed):** ğŸ“\n- Korrekte XML-Syntax (Ã¶ffnende/schlieÃŸende Tags stimmen Ã¼berein)\n- Ein Wurzel-Element vorhanden\n- Korrekte Verschachtelung der Elemente\n- Attribute in AnfÃ¼hrungszeichen\n\n**Valid (GÃ¼ltig):** âœ…\n- Wohlgeformt UND zusÃ¤tzlich konform zu DTD oder XML-Schema\n- Strukturregeln werden eingehalten\n- Erlaubte Elemente und Attribute werden verwendet\n\n**Hierarchie:** Jedes valide XML ist wohlgeformt, aber nicht jedes wohlgeformte XML ist valid! Ein XML kann syntaktisch korrekt sein, aber gegen die definierten GeschÃ¤ftsregeln verstoÃŸen ğŸ¯",
            "difficulty": "fortgeschritten",
            "category": "XML-Validierung"
        },
        {
            "question": "ğŸ—ï¸ Ein E-Commerce-System soll Produktdaten zwischen verschiedenen Abteilungen austauschen. Welche XML-Struktur ist fÃ¼r skalierbare Produktkataloge am besten geeignet?",
            "options": [
                "<products><product><name>Laptop</name><price>999</price></product></products>",
                "<catalog><category id='electronics'><product sku='LAP001'><name>Laptop</name><price currency='EUR'>999</price><stock>50</stock></product></category></catalog>",
                "<data><item>Laptop|999|EUR</item></data>",
                "<root><laptop>999</laptop></root>"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ—ï¸ Option B bietet die beste Struktur fÃ¼r Enterprise-Anwendungen:\n\n**Vorteile der gewÃ¤hlten Struktur:**\n- ğŸ·ï¸ **Eindeutige Identifikation:** SKU-Attribute fÃ¼r Produktreferenzen\n- ğŸ’° **WÃ¤hrungsunterstÃ¼tzung:** Currency-Attribut fÃ¼r internationale MÃ¤rkte\n- ğŸ“¦ **Lagerbestand:** Stock-Element fÃ¼r VerfÃ¼gbarkeitsprÃ¼fung\n- ğŸ“‚ **Kategorisierung:** Category-Container fÃ¼r Produktgruppierung\n- ğŸ” **Erweiterbarkeit:** Klare Struktur fÃ¼r zusÃ¤tzliche Attribute\n\n**Probleme anderer Optionen:**\n- Option A: Zu simpel, keine WÃ¤hrung/Kategorien\n- Option C: Pipe-separated Values in XML = Anti-Pattern\n- Option D: Produktspezifische Tags = nicht skalierbar ğŸš«",
            "difficulty": "fortgeschritten",
            "category": "XML-Anwendungsdesign"
        },
        {
            "question": "ğŸ’¾ Relationale Datenbanken speichern Daten in Tabellen. Was ist das Hauptmerkmal des relationalen Modells?",
            "options": [
                "Alle Daten werden in einer einzigen groÃŸen Tabelle gespeichert",
                "Tabellen sind Ã¼ber FremdschlÃ¼ssel miteinander verbunden",
                "Jede Tabelle kann nur maximal 255 Zeilen enthalten",
                "Die Reihenfolge der DatensÃ¤tze ist fest vorgegeben"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ’¾ Das relationale Datenbankmodell basiert auf der Verbindung (Relation) zwischen Tabellen:\n\n**Kernprinzipien relationaler Datenbanken:**\n- ğŸ”— **FremdschlÃ¼ssel-Beziehungen:** VerknÃ¼pfung zwischen Tabellen Ã¼ber gemeinsame Werte\n- ğŸ“Š **Normalisierung:** Daten werden auf mehrere Tabellen aufgeteilt, um Redundanzen zu vermeiden\n- ğŸ” **SQL-Abfragen:** JOIN-Operationen verbinden Daten aus verschiedenen Tabellen\n\n**Beispiel E-Commerce:**\n- Tabelle 'Kunden' (ID, Name, E-Mail)\n- Tabelle 'Bestellungen' (ID, Kunden_ID, Datum)\n- VerknÃ¼pfung Ã¼ber Kunden_ID ğŸ›’\n\n**Warum andere Optionen falsch sind:**\n- A: Eine Tabelle = nicht relational\n- C: Zeilenlimit ist implementierungsabhÃ¤ngig\n- D: SQL erlaubt flexible Sortierung",
            "difficulty": "einsteiger",
            "category": "Relationale-Datenbanken"
        },
        {
            "question": "ğŸ”§ Bei der Entwicklung eines datenbankbasierten Webshops mÃ¼ssen verschiedene Tabellen entworfen werden. Welche Normalisierungsregel sollte beachtet werden?",
            "options": [
                "Alle Produktinformationen in einer Tabelle speichern fÃ¼r bessere Performance",
                "Redundante Daten vermeiden durch Aufteilung in logisch getrennte Tabellen",
                "Jede Tabelle sollte mindestens 10 Spalten haben",
                "FremdschlÃ¼ssel nur in der Haupttabelle verwenden"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ”§ Die Normalisierung ist ein Kernprinzip relationaler Datenbankdesigns:\n\n**Vorteile der Normalisierung:**\n- ğŸš« **Redundanz-Vermeidung:** Jede Information wird nur einmal gespeichert\n- ğŸ”„ **Update-Anomalien vermeiden:** Ã„nderungen mÃ¼ssen nur an einer Stelle erfolgen\n- ğŸ’¾ **Speicherplatz-Effizienz:** Weniger doppelte Daten\n- ğŸ›¡ï¸ **DatenintegritÃ¤t:** Konsistenz durch zentrale Datenhaltung\n\n**Beispiel Webshop-Design:**\n- Tabelle 'Hersteller' (ID, Name, Land)\n- Tabelle 'Produkte' (ID, Name, Hersteller_ID, Preis)\n- Statt: Herstellername in jeder Produktzeile zu wiederholen\n\n**Warum andere Optionen problematisch sind:**\n- A: Eine Mega-Tabelle fÃ¼hrt zu Redundanzen und Update-Problemen\n- C: Spaltenanzahl ist nicht relevant fÃ¼r Design-QualitÃ¤t\n- D: FremdschlÃ¼ssel werden in abhÃ¤ngigen Tabellen benÃ¶tigt ğŸ¯",
            "difficulty": "fortgeschritten",
            "category": "Datenbankdesign"
        },
        {
            "question": "ğŸ“Š Welches Datenaustauschformat ist fÃ¼r die Ãœbertragung hierarchischer Produktkatalog-Daten zwischen verschiedenen E-Commerce-Systemen am besten geeignet?",
            "options": [
                "CSV, weil es einfacher zu parsen ist",
                "XML, wegen der hierarchischen Struktur und ValidierungsmÃ¶glichkeiten",
                "JSON, weil es weniger Speicherplatz benÃ¶tigt",
                "Plain Text, wegen der universellen KompatibilitÃ¤t"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ“Š XML ist ideal fÃ¼r hierarchische E-Commerce-Daten:\n\n**Vorteile von XML fÃ¼r Produktkataloge:**\n- ğŸŒ³ **Hierarchische Struktur:** Kategorien â†’ Unterkategorien â†’ Produkte â†’ Varianten\n- âœ… **Schema-Validierung:** DTD/XSD gewÃ¤hrleisten DatenqualitÃ¤t\n- ğŸ·ï¸ **Metadaten-Support:** Attribute fÃ¼r IDs, WÃ¤hrungen, Sprachen\n- ğŸ” **XPath-Abfragen:** Gezielte Datenextraktion mÃ¶glich\n- ğŸŒ **Standard-Compliance:** Viele E-Commerce-Standards basieren auf XML\n\n**Limitierungen anderer Formate:**\n- **CSV:** Flache Struktur, keine Hierarchien, keine Validierung ğŸ“ˆ\n- **JSON:** Weniger Validierungsoptionen, keine standardisierten Schemas fÃ¼r E-Commerce\n- **Plain Text:** Keine Struktur, keine Typisierung, fehleranfÃ¤llig ğŸ“\n\n**Praxis-Beispiel:** Amazon MWS, eBay API, viele PIM-Systeme nutzen XML-basierte Produktdatenformate! ğŸ›’",
            "difficulty": "fortgeschritten",
            "category": "Datenaustausch-Strategien"
        },
        {
            "question": "ğŸ¯ Ein XML-Parser meldet einen Fehler bei diesem Code: <song><title>Rock & Roll</title></song>. Was ist das Problem?",
            "options": [
                "Das Attribut 'nr' fehlt im song-Element",
                "Der Text 'Rock & Roll' enthÃ¤lt ein nicht-erlaubtes Sonderzeichen",
                "Die XML-Deklaration fehlt am Anfang",
                "Das schlieÃŸende Tag ist falsch geschrieben"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ¯ Das Ampersand-Zeichen (&) ist ein reserviertes Zeichen in XML:\n\n**XML-Reservierte Zeichen:** âš ï¸\n- `&` â†’ `&amp;` (Ampersand)\n- `<` â†’ `&lt;` (Kleiner als)\n- `>` â†’ `&gt;` (GrÃ¶ÃŸer als)\n- `\"` â†’ `&quot;` (AnfÃ¼hrungszeichen)\n- `'` â†’ `&apos;` (Apostroph)\n\n**Korrekte Version:**\n```xml\n<song><title>Rock &amp; Roll</title></song>\n```\n\n**Warum das passiert:** ğŸ¤–\n- XML-Parser interpretieren & als Beginn einer Entity-Referenz\n- Ohne nachfolgende gÃ¼ltige Entity (amp, lt, gt, etc.) entsteht ein Parse-Fehler\n- Dies ist ein hÃ¤ufiger Fehler bei Musikdatenbanken und Firmennames!\n\n**Andere Optionen sind falsch:**\n- A: Attribute sind optional (je nach DTD)\n- C: XML-Deklaration ist optional fÃ¼r wohlgeformtes XML\n- D: Tags sind korrekt geschrieben ğŸµ",
            "difficulty": "fortgeschritten",
            "category": "XML-Parsing-Fehler"
        },
        {
            "question": "ğŸ—‚ï¸ In einer DTD ist definiert: <!ELEMENT playlist (song+)>. Was bedeutet das fÃ¼r eine gÃ¼ltige XML-Datei?",
            "options": [
                "Ein playlist-Element kann beliebig viele oder keine song-Elemente enthalten",
                "Ein playlist-Element muss mindestens ein song-Element enthalten",
                "Ein playlist-Element kann maximal ein song-Element enthalten",
                "Ein playlist-Element muss genau drei song-Elemente enthalten"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ—‚ï¸ Das Plus-Zeichen (+) in DTD bedeutet \"ein oder mehrmals\":\n\n**DTD-HÃ¤ufigkeitsoperatoren:** ğŸ“‹\n- **Ohne Zeichen:** Genau einmal (z.B. `song`)\n- **?** Ein- oder keinmal / Optional (z.B. `song?`)\n- **+** Ein- oder mehrmals / Mindestens eines (z.B. `song+`) âœ…\n- **\\*** Kein-, ein- oder mehrmals / Beliebig oft (z.B. `song*`)\n\n**Praktische Bedeutung:**\n- Eine leere Playlist wÃ¤re NICHT valid (mindestens 1 Song erforderlich)\n- Eine Playlist mit 1, 5, 100 Songs ist valid\n- Dies macht Sinn: Eine Playlist ohne Songs ist logisch fragwÃ¼rdig! ğŸµ\n\n**Anwendungsbeispiel:**\n```xml\n<playlist>\n    <song>...</song>\n    <song>...</song>\n    <!-- Weitere Songs optional -->\n</playlist>\n```",
            "difficulty": "einsteiger",
            "category": "DTD-Syntax"
        },
        {
            "question": "ğŸ”„ Ein Webshop-System soll Produktdaten sowohl aus XML-Dateien als auch aus einer SQL-Datenbank verarbeiten. Welcher Ansatz ist fÃ¼r die Datenintegration optimal?",
            "options": [
                "XML-Daten direkt in HTML konvertieren ohne Zwischenspeicherung",
                "Alle XML-Daten in relationale Tabellen importieren und einheitlich mit SQL verarbeiten",
                "Zwei separate Systeme ohne Datenintegration betreiben",
                "Alle SQL-Daten nach XML exportieren und nur XML verwenden"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ”„ Die Konsolidierung in relationalen Tabellen bietet die beste LÃ¶sung:\n\n**Vorteile der SQL-zentrierten Integration:**\n- ğŸ¯ **Einheitliche Abfragen:** Eine Query-Sprache fÃ¼r alle Daten\n- âš¡ **Performance:** Indexierte Datenbankabfragen sind schneller als XML-Parsing\n- ğŸ” **Suchfunktionen:** Volltext-Suche, Filter, Sortierung Ã¼ber alle Produkte\n- ğŸ›¡ï¸ **Transaktionssicherheit:** ACID-Eigenschaften fÃ¼r konsistente Datenoperationen\n- ğŸ“Š **Reporting:** Business Intelligence Tools kÃ¶nnen direkt auf SQL-Daten zugreifen\n\n**ETL-Prozess (Extract, Transform, Load):**\n1. XML-Dateien parsen ğŸ“„\n2. Daten transformieren und normalisieren ğŸ”„\n3. In relationale Tabellen laden ğŸ’¾\n4. Einheitliche SQL-Abfragen fÃ¼r Frontend ğŸ–¥ï¸\n\n**Warum andere AnsÃ¤tze suboptimal sind:**\n- A: Keine Persistierung, keine Suchfunktionen\n- C: Datensilos fÃ¼hren zu inkonsistenten Informationen\n- D: Verlust der SQL-Vorteile bei komplexen Abfragen ğŸ›ï¸",
            "difficulty": "experte",
            "category": "Systemarchitektur"
        },
        {
            "question": "ğŸ“ Betrachten Sie diese XML-Struktur: <catalog><category name='electronics'><product/></category></catalog>. Was macht diese Struktur problematisch fÃ¼r den Datenaustausch?",
            "options": [
                "Das category-Element sollte ein id-Attribut statt name haben",
                "Das product-Element ist leer und enthÃ¤lt keine Daten",
                "Die Verschachtelung ist zu tief fÃ¼r XML-Parser",
                "Das catalog-Element sollte Attribute haben"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ“ Ein leeres `<product/>` Element ist strukturell problematisch:\n\n**Probleme leerer Elemente:**\n- ğŸš« **Keine Nutzdaten:** Produkt ohne Name, Preis, ID ist wertlos\n- ğŸ” **Parsing-Ineffizienz:** Parser verarbeitet strukturlose Elemente\n- ğŸ“Š **Import-Probleme:** Datenbank-Import kann nicht zwischen echten und leeren Produkten unterscheiden\n- ğŸ¯ **Business-Logik:** Leere Produkte kÃ¶nnen Fehler in der Anwendungslogik verursachen\n\n**Bessere Struktur:**\n```xml\n<catalog>\n    <category name='electronics'>\n        <product id='LAP001'>\n            <name>Gaming Laptop</name>\n            <price currency='EUR'>1299.99</price>\n        </product>\n    </category>\n</catalog>\n```\n\n**Andere Optionen sind weniger kritisch:**\n- A: name-Attribut ist akzeptabel (ID wÃ¤re besser, aber nicht falsch)\n- C: 3 Ebenen sind normale Verschachtelung\n- D: Catalog-Attribute sind optional ğŸ’»",
            "difficulty": "fortgeschritten",
            "category": "XML-DatenqualitÃ¤t"
        },
        {
            "question": "âš¡ Ein XML-basiertes Content Management System verarbeitet tÃ¤glich 10.000 Produktaktualisierungen. Welcher Ansatz optimiert die Performance?",
            "options": [
                "Alle XML-Dateien einzeln parsen und validieren",
                "XML-Streaming-Parser verwenden und inkrementelle Updates in die Datenbank schreiben",
                "XML in JSON konvertieren und dann verarbeiten",
                "Alle XML-Daten im Arbeitsspeicher laden und dort verarbeiten"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** âš¡ Bei High-Volume-Datenverarbeitung ist Streaming-basierte Architektur optimal:\n\n**Vorteile von XML-Streaming + Inkrementelle Updates:**\n- ğŸŒŠ **Memory-Effizienz:** SAX/StAX Parser laden nicht die gesamte XML-Datei in den RAM\n- ğŸš€ **Skalierbarkeit:** Konstanter Speicherverbrauch unabhÃ¤ngig von DateigrÃ¶ÃŸe\n- âš¡ **Performance:** Parallel processing mÃ¶glich wÃ¤hrend des Parsens\n- ğŸ”„ **Inkrementelle Updates:** Nur geÃ¤nderte DatensÃ¤tze werden in der DB aktualisiert\n- ğŸ’¾ **Transaktions-Batching:** Mehrere Updates in einer Transaktion = weniger DB-Overhead\n\n**Probleme anderer AnsÃ¤tze:**\n- A: **DOM-Parser:** 10.000 Ã— komplettes Parsen = exponentieller Zeitaufwand ğŸ“ˆ\n- C: **Format-Konvertierung:** ZusÃ¤tzlicher Overhead ohne Nutzen ğŸ”„\n- D: **RAM-Limiting:** Bei groÃŸen XML-Dateien kann der Arbeitsspeicher Ã¼berlasten ğŸ’¥\n\n**Praxis-Implementierung:** Apache Camel, Spring Batch oder Ã¤hnliche ETL-Frameworks nutzen genau diesen Ansatz! ğŸ› ï¸",
            "difficulty": "experte",
            "category": "Performance-Optimierung"
        },
        {
            "question": "ğŸ”— Eine relationale Datenbank fÃ¼r einen Webshop enthÃ¤lt die Tabellen 'Kunden', 'Bestellungen' und 'Produkte'. Welche SQL-Operation verbindet Daten aus mehreren Tabellen?",
            "options": [
                "SELECT mit WHERE-Klausel",
                "INSERT mit UNION",
                "JOIN mit ON-Bedingung",
                "UPDATE mit SET"
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ”— JOIN-Operationen sind das HerzstÃ¼ck relationaler Datenbanken:\n\n**JOIN-Typen fÃ¼r Webshop-Szenarien:**\n- ğŸ”— **INNER JOIN:** Nur DatensÃ¤tze mit Ãœbereinstimmungen (Bestellungen mit gÃ¼ltigen Kunden)\n- ğŸ“‹ **LEFT JOIN:** Alle DatensÃ¤tze der linken Tabelle (alle Kunden, auch ohne Bestellungen)\n- ğŸ“Š **RIGHT JOIN:** Alle DatensÃ¤tze der rechten Tabelle\n- ğŸ”„ **FULL OUTER JOIN:** Alle DatensÃ¤tze aus beiden Tabellen\n\n**Beispiel-Query:**\n```sql\nSELECT k.name, b.datum, p.produktname\nFROM Kunden k\nJOIN Bestellungen b ON k.kunden_id = b.kunden_id\nJOIN Produkte p ON b.produkt_id = p.produkt_id\n```\n\n**Warum andere Operationen nicht passen:**\n- A: WHERE filtert, verbindet aber keine Tabellen\n- B: UNION kombiniert Ergebnismengen vertikal, nicht horizontal\n- D: UPDATE Ã¤ndert Daten, liest aber nicht aus mehreren Tabellen ğŸ¯",
            "difficulty": "einsteiger",
            "category": "SQL-Grundlagen"
        },
        {
            "question": "ğŸ› ï¸ Sie mÃ¼ssen ein XML-Schema (XSD) erstellen, das sicherstellt, dass Produktpreise nur positive Dezimalzahlen mit maximal 2 Nachkommastellen sind. Welche XSD-Definition ist korrekt?",
            "options": [
                "<xs:element name='price' type='xs:string'/>",
                "<xs:element name='price' type='xs:decimal' minInclusive='0.01' fractionDigits='2'/>",
                "<xs:element name='price' type='xs:integer'/>",
                "<xs:element name='price' type='xs:float' pattern='[0-9]+\\.[0-9]{2}'/>"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ› ï¸ XSD bietet prÃ¤zise Datentyp-Validierung fÃ¼r Business-Logik:\n\n**Optimale XSD-Definition fÃ¼r Preise:**\n```xml\n<xs:element name='price' type='xs:decimal'>\n    <xs:restriction base='xs:decimal'>\n        <xs:minInclusive value='0.01'/>\n        <xs:fractionDigits value='2'/>\n    </xs:restriction>\n</xs:element>\n```\n\n**Warum diese Definition optimal ist:**\n- ğŸ’° **xs:decimal:** Exakte Dezimalzahlen ohne Rundungsfehler (im Gegensatz zu float)\n- âœ… **minInclusive='0.01':** Verhindert Null- oder Negativpreise\n- ğŸ¯ **fractionDigits='2':** Exakt 2 Nachkommastellen (Cent-Genauigkeit)\n- ğŸ›¡ï¸ **Business-Rule-Compliance:** Entspricht realen Preisanforderungen\n\n**Probleme anderer Optionen:**\n- A: String = keine numerische Validierung\n- C: Integer = keine Nachkommastellen mÃ¶glich\n- D: Float = Rundungsfehler + Pattern ist komplexer als nÃ¶tig ğŸ’¸",
            "difficulty": "experte",
            "category": "XML-Schema-Design"
        },
        {
            "question": "ğŸ“Š Wahr oder Falsch: CSV ist besser geeignet als XML fÃ¼r die Ãœbertragung relationaler Tabellendaten zwischen Datenbanksystemen.",
            "options": [
                "Wahr - CSV ist einfacher und effizienter",
                "Falsch - XML bietet mehr FlexibilitÃ¤t und Validierung",
                "Wahr - CSV hat weniger Overhead",
                "Falsch - CSV kann keine Beziehungen zwischen Tabellen darstellen"
            ],
            "correct": 3,
            "explain": "**BegrÃ¼ndung:** ğŸ“Š Die Aussage ist **FALSCH** - CSV hat fundamentale Limitierungen fÃ¼r relationale Daten:\n\n**CSV-Limitierungen bei relationalen Daten:** âš ï¸\n- ğŸš« **Keine FremdschlÃ¼ssel-Beziehungen:** Referentielle IntegritÃ¤t nicht darstellbar\n- ğŸ“‹ **Flache Struktur:** Nur eine Tabelle pro Datei\n- ğŸ”— **Keine JOIN-Information:** Beziehungen zwischen Tabellen gehen verloren\n- ğŸ’¾ **Datentyp-Verlust:** Alles wird als Text interpretiert\n- âŒ **Keine Constraints:** Keine Validierung von GeschÃ¤ftsregeln\n\n**XML-Vorteile fÃ¼r relationale Daten:**\n- ğŸŒ³ **Hierarchische Darstellung:** Parent-Child-Beziehungen mÃ¶glich\n- ğŸ” **Schema-Validierung:** XSD gewÃ¤hrleistet DatenintegritÃ¤t\n- ğŸ·ï¸ **Metadaten:** FremdschlÃ¼ssel als Attribute darstellbar\n- ğŸ¯ **Referentielle IntegritÃ¤t:** ID/IDREF-Mechanismen verfÃ¼gbar\n\n**Fazit:** FÃ¼r einfache, flache Tabellen ist CSV effizienter. FÃ¼r relationale Datenstrukturen mit Beziehungen ist XML die bessere Wahl! ğŸ›ï¸",
            "difficulty": "fortgeschritten",
            "category": "Format-Vergleich"
        },
        {
            "question": "ğŸµ Analysieren Sie dieses XML-Fragment: <album><song nr='1'/><song nr='1'/></album>. Welches Problem kÃ¶nnte in einer Musik-Datenbank auftreten?",
            "options": [
                "Die XML-Syntax ist fehlerhaft",
                "Doppelte Track-Nummern verletzen die Eindeutigkeit der Songpositionen",
                "Das album-Element benÃ¶tigt ein Attribut",
                "Die song-Elemente sind leer und daher ungÃ¼ltig"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸµ Doppelte Track-Nummern sind ein Business-Logic-Problem:\n\n**Warum doppelte Track-Nummern problematisch sind:**\n- ğŸ¯ **Eindeutigkeit:** Track-Nummer sollte Position im Album identifizieren\n- ğŸ” **Sortierprobleme:** Welcher Song ist wirklich \"Track 1\"?\n- ğŸ’¾ **Datenbank-Constraints:** Unique-Constraints wÃ¼rden verletzt\n- ğŸ§ **User Experience:** Playlist-Generierung wird mehrdeutig\n- ğŸ“Š **Reporting:** Verkaufsstatistiken pro Track werden verfÃ¤lscht\n\n**Bessere LÃ¶sung:**\n```xml\n<album id='ALB001'>\n    <song nr='1' id='S001'><title>Opening Song</title></song>\n    <song nr='2' id='S002'><title>Second Track</title></song>\n</album>\n```\n\n**XML-Schema-Validierung:**\n```xml\n<xs:unique name='unique-track-nr'>\n    <xs:selector xpath='song'/>\n    <xs:field xpath='@nr'/>\n</xs:unique>\n```\n\n**Andere Optionen sind falsch:**\n- A: Syntax ist korrekt (wohlgeformt)\n- C: Album-Attribute sind optional\n- D: Leere Elemente sind syntaktisch gÃ¼ltig ğŸ¼",
            "difficulty": "fortgeschritten",
            "category": "DatenintegritÃ¤t"
        },
        {
            "question": "âš™ï¸ Ein Content Management System importiert tÃ¤glich XML-Feeds von 50 verschiedenen Lieferanten. Jeder Lieferant verwendet ein anderes XML-Schema. Welche Architektur-Strategie ist optimal?",
            "options": [
                "FÃ¼r jeden Lieferanten einen eigenen XML-Parser entwickeln",
                "XSLT-Transformationen verwenden, um alle Formate in ein einheitliches Schema zu konvertieren",
                "Alle Lieferanten zwingen, dasselbe XML-Format zu verwenden",
                "Die XML-Daten direkt ohne Transformation in die Datenbank importieren"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** âš™ï¸ XSLT (eXtensible Stylesheet Language Transformations) ist die Standard-LÃ¶sung fÃ¼r XML-Format-Integration:\n\n**Vorteile der XSLT-basierten Architektur:**\n- ğŸ”„ **Format-Normalisierung:** Alle Eingabeformate â†’ Ein einheitliches Zielschema\n- ğŸ› ï¸ **Wartbarkeit:** Neue Lieferanten = Neue XSLT-Stylesheet hinzufÃ¼gen\n- âš¡ **Performance:** XSLT-Prozessoren sind hochoptimiert\n- ğŸ¯ **Skalierbarkeit:** Parallel processing verschiedener Transformationen\n- ğŸ“‹ **StandardkonformitÃ¤t:** W3C-Standard mit breiter Tool-UnterstÃ¼tzung\n\n**Typische ETL-Pipeline:**\n1. **Extract:** XML-Feeds von Lieferanten laden ğŸ“¥\n2. **Transform:** Lieferanten-spezifische XSLT anwenden ğŸ”„\n3. **Load:** Einheitliche XML-Struktur in Datenbank importieren ğŸ’¾\n\n**Warum andere AnsÃ¤tze problematisch sind:**\n- A: 50 Parser = 50Ã— Entwicklungs-/Wartungsaufwand\n- C: Lieferanten-Lock-in, verhandlungstechnisch unrealistisch\n- D: Schema-Chaos in der Datenbank, keine DatenqualitÃ¤t ğŸ›ï¸\n\n**Tool-Beispiele:** Saxon, Xalan, oder integrierte XSLT-Engines in Java/.NET ğŸ”§",
            "difficulty": "experte",
            "category": "Enterprise-Integration"
        },
        {
            "question": "ğŸ” Eine XML-Datei enthÃ¤lt sensible Kundendaten und soll validiert werden. Welche Kombination gewÃ¤hrleistet sowohl DatenintegritÃ¤t als auch Sicherheit?",
            "options": [
                "DTD-Validierung mit Klartextspeicherung",
                "XSD-Validierung mit XML-VerschlÃ¼sselung nach W3C XML Encryption Standard",
                "Keine Validierung, aber starke VerschlÃ¼sselung",
                "CSV-Export mit Passwort-Schutz"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ” Moderne XML-Security erfordert sowohl Struktur-Validierung als auch Datenschutz:\n\n**Warum XSD + XML Encryption optimal ist:**\n- âœ… **XSD-Validierung:** Strukturelle IntegritÃ¤t und Business-Rule-Compliance\n- ğŸ›¡ï¸ **W3C XML Encryption:** Industriestandard fÃ¼r selektive VerschlÃ¼sselung\n- ğŸ¯ **Selektive VerschlÃ¼sselung:** Nur sensible Elemente verschlÃ¼sselt, Metadaten bleiben suchbar\n- ğŸ” **Validierung vor VerschlÃ¼sselung:** Schema-Check auf Plaintext, dann VerschlÃ¼sselung\n- ğŸ”— **Tool-Integration:** Enterprise-XML-Tools unterstÃ¼tzen beide Standards nativ\n\n**XML Encryption Beispiel:**\n```xml\n<customer id='C001'>\n    <name>John Doe</name>\n    <EncryptedData xmlns='http://www.w3.org/2001/04/xmlenc#'>\n        <!-- VerschlÃ¼sselte Kreditkartendaten -->\n    </EncryptedData>\n</customer>\n```\n\n**Probleme anderer AnsÃ¤tze:**\n- A: DTD ist weniger mÃ¤chtig als XSD, keine VerschlÃ¼sselung\n- C: Struktur-Chaos ohne Validierung\n- D: CSV verliert XML-Vorteile, Passwort-Schutz ist schwÃ¤cher ğŸ”’",
            "difficulty": "experte",
            "category": "XML-Security"
        },
        {
            "question": "ğŸš€ Ein E-Commerce-API soll sowohl XML- als auch JSON-Responses unterstÃ¼tzen. Die Datenbank verwendet relationale Tabellen. Welche Middleware-Architektur ist optimal?",
            "options": [
                "Separate APIs fÃ¼r XML und JSON mit redundanter GeschÃ¤ftslogik",
                "Einheitliche GeschÃ¤ftslogik mit Content-Negotiation und Format-spezifischen Serializers",
                "Nur XML-API implementieren und client-seitig nach JSON konvertieren",
                "Alle Daten als XML in der Datenbank speichern"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸš€ Content-Negotiation mit einheitlicher GeschÃ¤ftslogik folgt REST-Best-Practices:\n\n**Optimale API-Architektur:**\n```\n[Client] â†’ [API Gateway] â†’ [Business Logic] â†’ [Database]\n    â†“            â†“               â†‘\n[Accept: application/xml] â†’ [XML Serializer]\n[Accept: application/json] â†’ [JSON Serializer]\n```\n\n**Architektur-Vorteile:**\n- ğŸ¯ **DRY-Prinzip:** Eine GeschÃ¤ftslogik fÃ¼r beide Formate\n- ğŸ”„ **Content Negotiation:** HTTP Accept-Header bestimmt Response-Format\n- âš¡ **Performance:** Datenbank-Queries nur einmal ausfÃ¼hren\n- ğŸ› ï¸ **Wartbarkeit:** Neue Formate (YAML, etc.) leicht hinzufÃ¼gbar\n- ğŸ“Š **Consistent Data:** Identische GeschÃ¤ftsregeln fÃ¼r alle Clients\n\n**HTTP Content-Negotiation:**\n```http\nGET /api/products/123\nAccept: application/xml â†’ XML Response\nAccept: application/json â†’ JSON Response\n```\n\n**Warum andere AnsÃ¤tze problematisch:**\n- A: Code-Duplikation, Inkonsistenz-Risiko\n- C: Client-Overhead, Netzwerk-Ineffizienz\n- D: SQL-Performance-Probleme, JSON ist fÃ¼r relationale Daten oft effizienter ğŸ›ï¸",
            "difficulty": "experte",
            "category": "API-Design"
        },
        {
            "question": "ğŸª In einem E-Commerce-System haben Sie eine Tabelle 'Kunden' mit 50.000 DatensÃ¤tzen und eine Tabelle 'Bestellungen' mit 200.000 DatensÃ¤tzen. Welche Beziehung liegt zwischen diesen Tabellen vor und wie wird sie technisch umgesetzt?",
            "options": [
                "1:1 Beziehung Ã¼ber zwei Primary Keys",
                "1:n Beziehung mit Kundennummer als Foreign Key in der Bestellungen-Tabelle",
                "n:m Beziehung Ã¼ber eine Zwischentabelle",
                "Keine direkte Beziehung, da unterschiedliche Datensatzanzahl"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ”— Dies ist eine klassische 1:n Beziehung, da ein Kunde mehrere Bestellungen haben kann, aber jede Bestellung zu genau einem Kunden gehÃ¶rt.\n\nğŸ“‹ **Technische Umsetzung:**\n- Primary Key der Kunden-Tabelle (z.B. KundenID) wird als Foreign Key in die Bestellungen-Tabelle eingefÃ¼gt\n- Dadurch kann jede Bestellung einem Kunden zugeordnet werden\n- Die unterschiedlichen Datensatzanzahlen bestÃ¤tigen die 1:n Beziehung (ein Kunde â†’ mehrere Bestellungen)\n\nâŒ **Warum andere Optionen falsch sind:**\n- Option A: 1:1 wÃ¼rde bedeuten, dass jeder Kunde nur eine Bestellung haben kann\n- Option C: n:m wÃ¤re nur nÃ¶tig, wenn eine Bestellung mehrere Kunden haben kÃ¶nnte\n- Option D: Unterschiedliche Datensatzanzahl ist bei 1:n normal und erwÃ¼nscht",
            "difficulty": "fortgeschritten",
            "category": "Datenbankdesign"
        },
        {
            "question": "ğŸ’¾ Berechnen Sie den Speicherplatzbedarf: Eine Tabelle hat 100.000 DatensÃ¤tze mit folgenden Spalten: INTEGER (4 Bytes), VARCHAR(50) (durchschnittlich 25 Zeichen), DATETIME (8 Bytes), FLOAT (4 Bytes). Wie viel Speicher benÃ¶tigt die reine Datenmenge?",
            "options": [
                "Etwa 4,1 MB",
                "Etwa 8,2 MB", 
                "Etwa 16,4 MB",
                "Etwa 2,05 MB"
            ],
            "correct": 0,
            "explain": "**BegrÃ¼ndung:** ğŸ§® **Berechnung pro Datensatz:**\n- INTEGER: 4 Bytes\n- VARCHAR(50): 25 Zeichen = 25 Bytes (bei ASCII)\n- DATETIME: 8 Bytes\n- FLOAT: 4 Bytes\n- **Summe pro Datensatz:** 4 + 25 + 8 + 4 = 41 Bytes\n\nğŸ“Š **Gesamtberechnung:**\n- 100.000 DatensÃ¤tze Ã— 41 Bytes = 4.100.000 Bytes\n- 4.100.000 Bytes Ã· 1.024 Ã· 1.024 â‰ˆ 3,91 MB â‰ˆ 4,1 MB\n\nâš ï¸ **Wichtiger Hinweis:** Dies ist nur die reine Datenmenge ohne Indexe, Metadaten oder Datenbank-Overhead, der in der Praxis deutlich hÃ¶her ausfÃ¤llt!",
            "difficulty": "fortgeschritten", 
            "category": "Datenbankoptimierung"
        },
        {
            "question": "ğŸ” Was passiert bei einem Versuch, einen Kunden zu lÃ¶schen, zu dem noch aktive Bestellungen existieren, wenn referenzielle IntegritÃ¤t aktiviert ist?",
            "options": [
                "Der Kunde wird gelÃ¶scht und alle zugehÃ¶rigen Bestellungen werden automatisch mitgelÃ¶scht",
                "Der LÃ¶schvorgang wird verhindert und eine Fehlermeldung ausgegeben",
                "Der Kunde wird gelÃ¶scht, die Bestellungen bleiben mit NULL-Werten bestehen",
                "Nur der Primary Key wird gelÃ¶scht, andere Daten bleiben erhalten"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ›¡ï¸ **Referenzielle IntegritÃ¤t schÃ¼tzt vor Dateninkonsistenz:**\n\nWenn referenzielle IntegritÃ¤t aktiviert ist, verhindert das DBMS den LÃ¶schvorgang und gibt eine Constraint-Violation-Fehlermeldung aus. Dies schÃ¼tzt vor 'verwaisten' DatensÃ¤tzen.\n\nğŸ”„ **LÃ¶sungsstrategien in der Praxis:**\n- **CASCADE DELETE:** Alle abhÃ¤ngigen DatensÃ¤tze werden mitgelÃ¶scht (gefÃ¤hrlich!)\n- **SET NULL:** Foreign Key-Werte werden auf NULL gesetzt (wenn erlaubt)\n- **RESTRICT:** LÃ¶schung wird verhindert (Standard-Verhalten)\n\nğŸ’¡ **Best Practice:** Erst abhÃ¤ngige DatensÃ¤tze prÃ¼fen/archivieren, dann Hauptdatensatz lÃ¶schen oder Soft-Delete verwenden (Status-Flag statt physische LÃ¶schung)",
            "difficulty": "fortgeschritten",
            "category": "DatenintegritÃ¤t"
        },
        {
            "question": "ğŸ—‚ï¸ Welcher Datentyp ist fÃ¼r die Speicherung von Produktpreisen in einem Shop-System am besten geeignet?",
            "options": [
                "FLOAT fÃ¼r maximale FlexibilitÃ¤t",
                "DECIMAL/NUMERIC fÃ¼r exakte GeldbetrÃ¤ge",
                "INTEGER fÃ¼r beste Performance",
                "VARCHAR fÃ¼r einfache Formatierung"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ’° **DECIMAL/NUMERIC ist die einzig korrekte Wahl fÃ¼r GeldbetrÃ¤ge!**\n\nğŸ¯ **Warum DECIMAL optimal ist:**\n- Exakte Dezimaldarstellung ohne Rundungsfehler\n- Festgelegte PrÃ¤zision (z.B. DECIMAL(10,2) fÃ¼r 99999999.99)\n- Standards-konforme GeldbetrÃ¤ge-Speicherung\n\nâŒ **Probleme anderer Datentypen:**\n- **FLOAT:** Rundungsfehler bei Berechnungen (0.1 + 0.2 â‰  0.3)\n- **INTEGER:** Nur ganze Zahlen, keine Cent-BetrÃ¤ge mÃ¶glich\n- **VARCHAR:** Keine mathematischen Operationen, keine Sortierung\n\nâš¡ **Praxis-Tipp:** Manche Systeme speichern CentbetrÃ¤ge als INTEGER (1299 = 12,99â‚¬) fÃ¼r maximale Genauigkeit",
            "difficulty": "einsteiger",
            "category": "Datentypen"
        },
        {
            "question": "ğŸ”„ Sie haben eine n:m Beziehung zwischen 'Studenten' und 'Kurse'. Wie lÃ¶sen Sie diese korrekt auf?",
            "options": [
                "Zwei separate 1:1 Beziehungen erstellen",
                "Eine Zwischentabelle mit beiden Primary Keys als Foreign Keys",
                "Den Primary Key der einen Tabelle in die andere als Array speichern",
                "Eine JSON-Spalte fÃ¼r die Zuordnungen verwenden"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ“ **Zwischentabelle ist die Standard-LÃ¶sung fÃ¼r n:m Beziehungen:**\n\nğŸ“‹ **Aufbau der Zwischentabelle 'Student_Kurs':**\n- StudentID (Foreign Key zu Studenten-Tabelle)\n- KursID (Foreign Key zu Kurse-Tabelle)\n- Optional: Zusatzattribute (Anmeldedatum, Note)\n- Kombinierter Primary Key aus beiden Foreign Keys\n\nğŸ”— **Dadurch entstehen zwei 1:n Beziehungen:**\n- Ein Student â†’ viele EintrÃ¤ge in Student_Kurs (verschiedene Kurse)\n- Ein Kurs â†’ viele EintrÃ¤ge in Student_Kurs (verschiedene Studenten)\n\nâœ… **Vorteile:** Redundanzfrei, erweiterbar, SQL-Standard-konform\nâŒ **Warum andere falsch:** Arrays/JSON verletzen erste Normalform, 1:1 lÃ¶st n:m nicht auf",
            "difficulty": "fortgeschritten",
            "category": "Datenbankdesign"
        },
        {
            "question": "âš¡ In welcher Reihenfolge sollten Sie vorgehen, um die Datenkonsistenz beim EinfÃ¼gen eines neuen Datensatzes mit Foreign Key-Referenzen zu gewÃ¤hrleisten?",
            "options": [
                "Foreign Key-DatensÃ¤tze zuerst, dann Primary Key-Datensatz",
                "Primary Key-Datensatz zuerst, dann Foreign Key-DatensÃ¤tze", 
                "Beide gleichzeitig in einer Transaktion",
                "Reihenfolge ist irrelevant bei aktivierten Constraints"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ¯ **Primary Key-Datensatz muss immer zuerst existieren!**\n\nğŸ“ **Logische Reihenfolge:**\n1. **Erst:** Datensatz in Tabelle mit Primary Key einfÃ¼gen\n2. **Dann:** DatensÃ¤tze mit Foreign Key-Referenzen auf diesen PK\n\nğŸ”’ **Warum diese Reihenfolge zwingend:**\n- Foreign Key kann nur auf existierenden Primary Key verweisen\n- Referenzielle IntegritÃ¤t verhindert 'tote Verweise'\n- DBMS prÃ¼ft FK-Constraints beim INSERT\n\nğŸ’¡ **Praxis-Beispiel Shop:**\n1. Kunde anlegen (erhÃ¤lt KundenID)\n2. Bestellung mit dieser KundenID als FK anlegen\n\nâš ï¸ **Bei umgekehrter Reihenfolge:** Constraint-Violation-Error, da FK auf nicht-existenten PK zeigt",
            "difficulty": "fortgeschritten",
            "category": "DatenintegritÃ¤t"
        },
        {
            "question": "ğŸ—ï¸ Warum ist Redundanzfreiheit in relationalen Datenbanken so wichtig?",
            "options": [
                "Nur um Speicherplatz zu sparen",
                "Um Anomalien bei Ã„nderungs-, EinfÃ¼ge- und LÃ¶schoperationen zu vermeiden",
                "Zur Verbesserung der Abfrage-Performance",
                "Wegen gesetzlicher Datenschutz-Bestimmungen"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸš¨ **Redundanz fÃ¼hrt zu gefÃ¤hrlichen Anomalien:**\n\nâš ï¸ **Update-Anomalie:** Daten an mehreren Stellen Ã¤ndern â†’ Inkonsistenz mÃ¶glich\n- Beispiel: Kundenadresse in 5 Tabellen â†’ vergisst man eine, entstehen WidersprÃ¼che\n\nğŸ”„ **Insert-Anomalie:** Daten kÃ¶nnen nur mit irrelevanten anderen Daten gespeichert werden\n- Beispiel: Neue Abteilung nur mit Mitarbeiter anlegbar\n\nğŸ—‘ï¸ **Delete-Anomalie:** Wichtige Daten gehen ungewollt verloren\n- Beispiel: Letzter Mitarbeiter gelÃ¶scht â†’ Abteilungsinfo weg\n\nğŸ’¾ **Speicherplatz ist Nebeneffekt**, nicht Hauptgrund!\nğŸƒâ€â™‚ï¸ **Performance:** Redundanzfreiheit kann sogar langsamer sein (mehr JOINs)\nğŸ“‹ Normalisierung lÃ¶st diese Probleme durch Aufteilung in verknÃ¼pfte Tabellen",
            "difficulty": "einsteiger",
            "category": "Datenbanktheorie"
        },
        {
            "question": "ğŸ” Sie haben eine Tabelle 'Mitarbeiter' mit 10.000 DatensÃ¤tzen. Der Primary Key ist die Personalnummer (INTEGER). Welche Eigenschaften muss dieser PK erfÃ¼llen?",
            "options": [
                "Eindeutig, nicht NULL, unverÃ¤nderlich, mÃ¶glichst klein",
                "Eindeutig, kann NULL sein, verÃ¤nderbar, aussagekrÃ¤ftig", 
                "Eindeutig, nicht NULL, aber verÃ¤nderbar und lang",
                "Nur eindeutig und numerisch"
            ],
            "correct": 0,
            "explain": "**BegrÃ¼ndung:** ğŸ¯ **Primary Key Eigenschaften (ACID-konform):**\n\nâœ… **Eindeutig (UNIQUE):** Keine Duplikate erlaubt\n- Jeder Mitarbeiter braucht einzigartige Personalnummer\n\nğŸš« **Nicht NULL:** Primary Key darf nie leer sein\n- Sonst keine eindeutige Identifikation mÃ¶glich\n\nğŸ”’ **UnverÃ¤nderlich:** PK sollte sich nie Ã¤ndern\n- Foreign Key-Referenzen wÃ¼rden sonst ungÃ¼ltig\n- Historische DatenintegritÃ¤t gewÃ¤hrleistet\n\nâš¡ **MÃ¶glichst klein:** Bessere Performance bei Indexierung und JOINs\n- INTEGER (4 Bytes) besser als VARCHAR(50)\n\nâŒ **Warum andere falsch:**\n- NULL-Werte im PK verletzen relationale Regeln\n- VerÃ¤nderbare PKs gefÃ¤hrden DatenintegritÃ¤t\n- Aussagekraft ist bei PKs nicht erforderlich (Surrogate Keys)",
            "difficulty": "fortgeschritten",
            "category": "Datenbankdesign"
        },
        {
            "question": "ğŸ“Š Wahr oder Falsch: 'In einer normalisierten Datenbank sollten alle Tabellen mindestens zwei Spalten haben.'",
            "options": [
                "Wahr - Mindestens PK und ein Attribut sind erforderlich",
                "Falsch - Eine Spalte (nur PK) kann in speziellen FÃ¤llen sinnvoll sein",
                "Wahr - Sonst wÃ¤re es keine relationale Tabelle",
                "Falsch - Tabellen kÃ¶nnen auch ganz ohne Spalten existieren"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** âŒ **FALSCH** - Ein-Spalten-Tabellen sind in speziellen FÃ¤llen durchaus sinnvoll!\n\nğŸ¯ **Legitime Ein-Spalten-Tabellen:**\n- **Lookup-Tabellen:** Status-Werte, Kategorien, LÃ¤nder-Codes\n- **Domain-Tabellen:** GÃ¼ltige Werte definieren (z.B. PrioritÃ¤ten: 'Hoch', 'Mittel', 'Niedrig')\n- **Referenz-Tabellen:** Master-Daten ohne zusÃ¤tzliche Attribute\n\nğŸ“‹ **Beispiel Tabelle 'Abteilungen':**\n```sql\nCREATE TABLE Abteilungen (\n    AbteilungsName VARCHAR(50) PRIMARY KEY\n);\n```\n\nâœ… **Vorteile:** Referenzielle IntegritÃ¤t, normalisiert, wartbar\nğŸ”— **Verwendung:** Als FK-Ziel in anderen Tabellen\n\nğŸ’¡ **Fazit:** Anzahl Spalten richtet sich nach fachlichen Anforderungen, nicht nach formalen Regeln",
            "difficulty": "experte",
            "category": "Datenbanktheorie"
        },
        {
            "question": "ğŸ”§ Ein Entwickler beschwert sich Ã¼ber langsame Abfragen in einer Tabelle mit 1 Million DatensÃ¤tzen. Die WHERE-Klausel filtert nach einer Spalte ohne Index. Was schlagen Sie vor?",
            "options": [
                "Mehr RAM einbauen",
                "Einen Index auf die gefilterte Spalte erstellen",
                "Die Tabelle in kleinere Tabellen aufteilen",
                "Einen schnelleren Prozessor verwenden"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸš€ **Index ist die effektivste LÃ¶sung!**\n\nâš¡ **Warum Index hilft:**\n- **Ohne Index:** Full Table Scan durch 1 Million Zeilen (O(n))\n- **Mit Index:** Logarithmische Suche (O(log n))\n- **Performance-Gewinn:** Von Sekunden auf Millisekunden\n\nğŸ“Š **Zahlenbeispiel:**\n- 1.000.000 Zeilen ohne Index: ~1.000.000 LesevorgÃ¤nge\n- Mit B-Tree Index: ~20 LesevorgÃ¤nge (logâ‚‚ 1.000.000)\n\nğŸ¯ **Index-Erstellung:**\n```sql\nCREATE INDEX idx_spaltenname ON tabelle(spaltenname);\n```\n\nâŒ **Warum andere Optionen nicht helfen:**\n- RAM/CPU: Beschleunigen Full Scan nur minimal\n- Tabellen-Aufteilung: Komplexere Architektur ohne echten Nutzen\n\nâš ï¸ **Achtung:** Indexe verlangsamen INSERT/UPDATE/DELETE-Operationen leicht",
            "difficulty": "fortgeschritten",
            "category": "Datenbankoptimierung"
        },
        {
            "question": "ğŸŒ Vergleichen Sie die Datentypen TIMESTAMP und DATETIME fÃ¼r die Speicherung von Bestellzeiten in einem globalen E-Commerce-System:",
            "options": [
                "DATETIME ist besser, da es Zeitzone-unabhÃ¤ngig ist",
                "TIMESTAMP ist besser, da es automatisch UTC speichert und Zeitzonen-konvertierung unterstÃ¼tzt",
                "Beide sind identisch in ihrer FunktionalitÃ¤t",
                "VARCHAR ist fÃ¼r globale Systeme am besten geeignet"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸŒ **TIMESTAMP ist fÃ¼r globale Systeme optimal:**\n\nğŸ• **TIMESTAMP Vorteile:**\n- Automatische UTC-Speicherung (Zeitzone-neutral)\n- Session-Zeitzone wird bei Ausgabe berÃ¼cksichtigt\n- Kleinerer Speicherbedarf (4 vs. 8 Bytes bei MySQL)\n- Automatische Updates bei Ã„nderungen mÃ¶glich\n\nğŸ“… **DATETIME Nachteile in globalen Systemen:**\n- Keine automatische Zeitzone-Behandlung\n- Speichert 'lokale' Zeit ohne Zeitzone-Info\n- Probleme bei Kunden in verschiedenen Zeitzonen\n\nğŸŒ **Praxis-Beispiel:**\n- Kunde aus Tokyo bestellt um 15:00 JST\n- TIMESTAMP: Speichert als 06:00 UTC\n- DATETIME: Speichert als 15:00 (ohne Zeitzone-Info)\n\nğŸ’¡ **Best Practice:** TIMESTAMP + Zeitzone-Spalte fÃ¼r maximale FlexibilitÃ¤t",
            "difficulty": "experte",
            "category": "Datentypen"
        },
        {
            "question": "ğŸ”„ In einem CRM-System haben Sie: Tabelle 'Personen' (50.000), 'Unternehmen' (5.000), 'Kontakte' (200.000). Eine Person kann zu mehreren Unternehmen gehÃ¶ren. Wie modellieren Sie diese Beziehung optimal?",
            "options": [
                "Direkte n:m Beziehung zwischen Personen und Unternehmen",
                "Zwischentabelle 'Person_Unternehmen' mit zusÃ¤tzlichen Attributen (Position, Startdatum)",
                "UnternehmensID als Array in der Personen-Tabelle",
                "Separate Tabelle fÃ¼r jede Person-Unternehmen-Kombination"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ’¼ **Zwischentabelle mit Zusatzattributen ist die professionelle LÃ¶sung:**\n\nğŸ—ï¸ **Tabelle 'Person_Unternehmen' Struktur:**\n- PersonID (FK zu Personen)\n- UnternehmensID (FK zu Unternehmen)\n- Position (VARCHAR) - z.B. 'Consultant', 'Manager'\n- Startdatum (DATE)\n- Enddatum (DATE, nullable)\n- Kombinierter PK aus PersonID + UnternehmensID\n\nâœ… **Vorteile dieser LÃ¶sung:**\n- Relationale IntegritÃ¤t gewÃ¤hrleistet\n- Erweiterbar um GeschÃ¤fts-relevante Attribute\n- Historische Daten mÃ¶glich (ZeitrÃ¤ume)\n- Standard SQL-Abfragen\n\nğŸ“Š **Skalierung:** 200.000 Kontakte deutet auf komplexe GeschÃ¤ftsbeziehungen hin\nâŒ **Arrays verletzen** erste Normalform und sind schwer abfragbar\nğŸ¯ **RealitÃ¤tsnah:** Personen haben unterschiedliche Rollen in verschiedenen Unternehmen",
            "difficulty": "experte",
            "category": "Datenbankdesign"
        },
        {
            "question": "ğŸ› ï¸ Welche SQL-Aussage Ã¼ber die Behandlung von NULL-Werten ist korrekt?",
            "options": [
                "NULL = NULL ergibt immer TRUE",
                "NULL ist dasselbe wie eine leere Zeichenkette ('')",
                "NULL bedeutet 'unbekannt' und verhÃ¤lt sich bei Vergleichen speziell",
                "NULL-Werte werden bei COUNT(*) nicht mitgezÃ¤hlt"
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** â“ **NULL reprÃ¤sentiert 'unbekannt/nicht vorhanden' und folgt 3-wertiger Logik:**\n\nğŸ§® **NULL-Verhalten bei Vergleichen:**\n- `NULL = NULL` â†’ UNKNOWN (nicht TRUE!)\n- `5 > NULL` â†’ UNKNOWN\n- `NULL + 10` â†’ NULL\n- Nur `IS NULL` und `IS NOT NULL` funktionieren\n\nğŸ“Š **COUNT-Verhalten:**\n- `COUNT(*)` zÃ¤hlt alle Zeilen (inkl. NULL)\n- `COUNT(spalte)` ignoriert NULL-Werte\n\nâŒ **HÃ¤ufige MissverstÃ¤ndnisse:**\n- NULL â‰  0 (Zahl)\n- NULL â‰  '' (leere Zeichenkette) \n- NULL â‰  FALSE (Boolean)\n\nğŸ’¡ **Praxis-Tipp:** WHERE-Klauseln mit NULL erfordern explizite IS NULL/IS NOT NULL Behandlung\n\nğŸ¯ **3-wertige Logik:** TRUE, FALSE, UNKNOWN (NULL-Vergleiche)",
            "difficulty": "fortgeschritten",
            "category": "SQL-Grundlagen"
        },
        {
            "question": "ğŸ“ˆ Sie analysieren die Performance einer JOIN-Operation zwischen zwei groÃŸen Tabellen (je 500.000 DatensÃ¤tze). Was ist der wichtigste Faktor fÃ¼r optimale Performance?",
            "options": [
                "Die GrÃ¶ÃŸe des verfÃ¼gbaren RAMs",
                "Indexe auf den JOIN-Spalten beider Tabellen",
                "Die Reihenfolge der Tabellen in der FROM-Klausel",
                "Die Verwendung von INNER statt LEFT JOIN"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ¯ **Indexe auf JOIN-Spalten sind Performance-kritisch!**\n\nâš¡ **Warum Indexe entscheidend sind:**\n- **Ohne Indexe:** Nested Loop Join = 500.000 Ã— 500.000 = 250 Milliarden Vergleiche\n- **Mit Indexen:** Hash/Merge Join = deutlich weniger Operationen\n- **Performance-Faktor:** 1000x oder mehr Beschleunigung mÃ¶glich\n\nğŸ” **JOIN-Algorithmen nutzen Indexe:**\n- **Nested Loop:** Index auf innerer Tabelle\n- **Hash Join:** Index hilft bei Erstellung der Hash-Tabelle\n- **Merge Join:** Sortierte Indexe ermÃ¶glichen effizienten Merge\n\nğŸ“Š **Andere Faktoren sind sekundÃ¤r:**\n- RAM: Hilft beim Caching, lÃ¶st aber nicht das Grundproblem\n- Tabellen-Reihenfolge: Query Optimizer entscheidet meist optimal\n- JOIN-Typ: Performance-Unterschied minimal bei gleichen Indexen\n\nğŸ’¡ **Best Practice:** Composite Index auf hÃ¤ufig verknÃ¼pfte Spalten",
            "difficulty": "experte",
            "category": "Datenbankoptimierung"
        },
        {
            "question": "ğŸª In einem Warenwirtschaftssystem soll verhindert werden, dass der Lagerbestand negativ wird. Welcher Ansatz ist am robustesten?",
            "options": [
                "Check Constraint auf der Lagerbestand-Spalte",
                "Trigger, der vor Updates den Bestand prÃ¼ft",
                "Anwendungslogik prÃ¼ft vor jeder Buchung",
                "Stored Procedure fÃ¼r alle Lagerbewegungen"
            ],
            "correct": 0,
            "explain": "**BegrÃ¼ndung:** ğŸ›¡ï¸ **Check Constraint ist die sicherste LÃ¶sung auf Datenschicht:**\n\nâœ… **Vorteile von Check Constraints:**\n- **Unvermeidlich:** Wird IMMER geprÃ¼ft, egal welche Anwendung zugreift\n- **Performance:** Sehr schnell, da in DB-Engine integriert\n- **Deklarativ:** Regel ist in Schema dokumentiert\n- **Transaktions-sicher:** Rollback bei Verletzung\n\n```sql\nALTER TABLE Lager \nADD CONSTRAINT chk_bestand_positiv \nCHECK (bestand >= 0);\n```\n\nâŒ **Probleme anderer AnsÃ¤tze:**\n- **Trigger:** KÃ¶nnen deaktiviert werden, komplexer zu debuggen\n- **Anwendungslogik:** Kann umgangen werden, Race Conditions mÃ¶glich\n- **Stored Procedures:** KÃ¶nnen umgangen werden, nicht automatisch\n\nğŸ¯ **Defense in Depth:** Check Constraint + Anwendungslogik fÃ¼r beste UX\nâš ï¸ **Achtung:** Constraint-Verletzung fÃ¼hrt zu Fehlern - UI sollte das abfangen",
            "difficulty": "fortgeschritten",
            "category": "DatenintegritÃ¤t"
        },
        {
            "question": "ğŸ”§ Analysieren Sie dieses Szenario: Eine Tabelle 'AuftrÃ¤ge' hat sowohl 'erstellt_am' als auch 'bearbeitet_am' Spalten. Wie sollten diese fÃ¼r optimale Wartbarkeit typisiert sein?",
            "options": [
                "Beide als VARCHAR fÃ¼r flexible Formatierung",
                "erstellt_am als TIMESTAMP (automatisch), bearbeitet_am als DATETIME (manuell)",
                "Beide als TIMESTAMP mit DEFAULT-Werten",
                "erstellt_am als DATE, bearbeitet_am als TIME"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** â° **Unterschiedliche Datentypen fÃ¼r unterschiedliche Zwecke:**\n\nğŸ¯ **Optimale Typisierung:**\n- **erstellt_am:** `TIMESTAMP DEFAULT CURRENT_TIMESTAMP`\n  - Automatisch beim INSERT gesetzt\n  - UTC-Zeitzone fÃ¼r globale Systeme\n  - UnverÃ¤nderlich nach Erstellung\n\n- **bearbeitet_am:** `DATETIME NULL`\n  - Explizit durch Anwendung gesetzt\n  - Kann NULL sein (noch nicht bearbeitet)\n  - FlexibilitÃ¤t bei manueller Zeitangabe\n\nğŸ’¡ **Wartbarkeits-Vorteile:**\n```sql\nCREATE TABLE Auftraege (\n    id INT PRIMARY KEY,\n    erstellt_am TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    bearbeitet_am DATETIME NULL\n);\n```\n\nâŒ **Warum andere falsch:**\n- VARCHAR: Keine Sortierung, Formatierungs-Chaos\n- Beide TIMESTAMP: Zu restriktiv fÃ¼r manuelle Bearbeitung\n- DATE/TIME: Verlust der Tageszeit bzw. Datum-Kontext",
            "difficulty": "experte",
            "category": "Datentypen"
        },
        {
            "question": "ğŸ” Was bedeutet es, wenn eine Datenbank in der 3. Normalform (3NF) ist?",
            "options": [
                "Alle Tabellen haben maximal 3 Spalten",
                "Keine transitiven AbhÃ¤ngigkeiten von Nicht-SchlÃ¼ssel-Attributen",
                "Maximal 3 FremdschlÃ¼ssel pro Tabelle",
                "Die Datenbank hat hÃ¶chstens 3 Tabellen"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ“ **3NF eliminiert transitive AbhÃ¤ngigkeiten:**\n\nğŸ¯ **Definition 3. Normalform:**\n- ErfÃ¼llt 1NF (atomare Werte) und 2NF (keine partiellen AbhÃ¤ngigkeiten)\n- **ZusÃ¤tzlich:** Keine transitiven AbhÃ¤ngigkeiten von Nicht-SchlÃ¼ssel-Attributen\n\nğŸ”„ **Transitive AbhÃ¤ngigkeit Beispiel:**\n**Schlecht:** Tabelle Mitarbeiter (ID, Name, AbteilungsID, AbteilungsLeiter)\n- Name abhÃ¤ngig von ID âœ…\n- AbteilungsID abhÃ¤ngig von ID âœ…\n- AbteilungsLeiter abhÃ¤ngig von AbteilungsID âŒ (transitive AbhÃ¤ngigkeit!)\n\nâœ… **3NF-LÃ¶sung:** Separate Tabelle Abteilungen (AbteilungsID, AbteilungsLeiter)\nğŸ’¡ **Vorteil:** Keine Redundanz, konsistente Daten, wartbarer Code",
            "difficulty": "fortgeschritten",
            "category": "Datenbanktheorie"
        },
        {
            "question": "âš™ï¸ Ein System protokolliert Benutzer-AktivitÃ¤ten. Bei 100.000 Benutzern und durchschnittlich 50 Aktionen pro Tag entsteht eine Log-Tabelle mit 5 Millionen EintrÃ¤gen tÃ¤glich. Welche Strategie ist fÃ¼r langfristige Performance am besten?",
            "options": [
                "Alle Daten in einer groÃŸen Tabelle mit Indexen",
                "Partitionierung nach Datum mit automatischer Archivierung",
                "Separate Tabelle fÃ¼r jeden Benutzer",
                "NoSQL-Datenbank fÃ¼r Log-Daten verwenden"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ“… **Partitionierung nach Datum ist optimal fÃ¼r Log-Daten:**\n\nğŸ¯ **Warum Datum-Partitionierung ideal ist:**\n- **Performance:** Nur relevante Partitionen werden abgefragt\n- **Wartung:** Alte Partitionen kÃ¶nnen automatisch gelÃ¶scht werden\n- **Speicher:** Alte Daten auf langsamere/gÃ¼nstigere Medien\n- **Backup:** Partition-weise Sicherung mÃ¶glich\n\nğŸ“Š **Zahlen-Beispiel:**\n- 5 Mio. EintrÃ¤ge/Tag Ã— 365 Tage = 1.8 Mrd. EintrÃ¤ge/Jahr\n- Mit Partitionierung: Aktuelle Abfragen nur auf wenige Partitionen\n- Ohne: Full Table Scan durch Milliarden von EintrÃ¤gen\n\nğŸ—“ï¸ **Implementierung:**\n```sql\nCREATE TABLE logs (id, user_id, action, timestamp)\nPARTITION BY RANGE (timestamp);\n```\n\nâŒ **Andere Optionen:** Skalieren nicht, NoSQL verliert relationale Vorteile",
            "difficulty": "experte",
            "category": "Datenbankoptimierung"
        },
        {
            "question": "ğŸ” Welche Aussage Ã¼ber Composite Primary Keys (zusammengesetzte PrimÃ¤rschlÃ¼ssel) ist richtig?",
            "options": [
                "Sie bestehen immer aus genau zwei Spalten",
                "Sie sind langsamer als einfache Primary Keys und sollten vermieden werden",
                "Sie sind sinnvoll bei Zwischentabellen fÃ¼r n:m Beziehungen",
                "Sie kÃ¶nnen NULL-Werte in einer der Spalten enthalten"
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ”— **Composite Primary Keys sind bei Zwischentabellen Standard:**\n\nâœ… **Perfekter Use Case - Student_Kurs Tabelle:**\n```sql\nCREATE TABLE Student_Kurs (\n    StudentID INT,\n    KursID INT,\n    Anmeldedatum DATE,\n    PRIMARY KEY (StudentID, KursID)\n);\n```\n\nğŸ¯ **Vorteile bei n:m AuflÃ¶sung:**\n- NatÃ¼rliche Eindeutigkeit (Student X in Kurs Y nur einmal)\n- Keine kÃ¼nstliche ID nÃ¶tig\n- Beide FKs sind gleichzeitig PK\n- Fachlich logisch und verstÃ¤ndlich\n\nâŒ **HÃ¤ufige MissverstÃ¤ndnisse:**\n- KÃ¶nnen 2, 3 oder mehr Spalten umfassen (nicht nur 2)\n- Performance ist meist vernachlÃ¤ssigbar schlechter\n- KEINE NULL-Werte erlaubt (PK-Regel)\n\nâš¡ **Best Practice:** Composite PK fÃ¼r natÃ¼rliche GeschÃ¤ftsschlÃ¼ssel, Surrogate Key fÃ¼r technische IDs",
            "difficulty": "fortgeschritten",
            "category": "Datenbankdesign"
        },
        {
            "question": "ğŸ“ Sie mÃ¼ssen in einer Tabelle sowohl die Original-Reihenfolge der Dateneingabe als auch die alphabetische Sortierung nach Name unterstÃ¼tzen. Wie lÃ¶sen Sie das elegant?",
            "options": [
                "Zwei separate Tabellen mit identischen Daten",
                "Eine zusÃ¤tzliche Spalte 'reihenfolge' mit AUTO_INCREMENT", 
                "TIMESTAMP-Spalte fÃ¼r die Eingabezeit verwenden",
                "Die Sortierung nur in der Anwendungsschicht verwalten"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ”¢ **AUTO_INCREMENT Spalte ist die eleganteste LÃ¶sung:**\n\nâœ… **Optimale Implementierung:**\n```sql\nCREATE TABLE eintraege (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    reihenfolge INT AUTO_INCREMENT,\n    name VARCHAR(100),\n    weitere_daten...,\n    UNIQUE KEY uk_reihenfolge (reihenfolge)\n);\n```\n\nğŸ¯ **Vorteile dieser LÃ¶sung:**\n- **Original-Reihenfolge:** `ORDER BY reihenfolge`\n- **Alphabetisch:** `ORDER BY name`\n- **Eindeutig:** Jeder Datensatz hat einzigartige Reihenfolge\n- **Performance:** Beide Sortierungen Ã¼ber Index mÃ¶glich\n\nâ° **Warum nicht TIMESTAMP:**\n- Mehrere EintrÃ¤ge zur gleichen Millisekunde mÃ¶glich\n- Datum/Zeit nicht immer relevant\n- GrÃ¶ÃŸerer Speicherbedarf\n\nğŸ’¡ **Praxis-Tipp:** Bei nachtrÃ¤glichen Ã„nderungen der Reihenfolge UPDATE mit neuen reihenfolge-Werten",
            "difficulty": "fortgeschritten",
            "category": "Datenbankdesign"
        },
        {
            "question": "Ein Musiker ğŸ¸, der noch keine Alben verÃ¶ffentlicht hat, mÃ¶chte sich in die Datenbank eintragen lassen. Basierend auf der unnormalisierten Tabelle aus dem Handout, welche der drei Anomalien hindert ihn daran, seine Daten hinzuzufÃ¼gen?",
            "options": [
                "Ã„nderungsanomalie (Update Anomaly)",
                "LÃ¶schanomalie (Deletion Anomaly)",
                "EinfÃ¼geanomalie (Insertion Anomaly)",
                "Redundanzanomalie (Redundancy Anomaly)"
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** Die EinfÃ¼geanomalie tritt auf, wenn man eine EntitÃ¤t (z. B. eine Band) nicht hinzufÃ¼gen kann, ohne gleichzeitig Daten zu einer anderen, abhÃ¤ngigen EntitÃ¤t (z. B. einem Album) anzugeben. ğŸ“ Die unnormalisierte Tabelle erfordert fÃ¼r jeden Datensatz einen Albumtitel und Track. Da der Musiker noch keine Alben hat, kann er nicht ohne weiteres eingefÃ¼gt werden. Die LÃ¶schanomalie betrifft das unbeabsichtigte LÃ¶schen von Daten, die Ã„nderungsanomalie das Problem inkonsistenter Daten bei Ã„nderungen. Die Redundanzanomalie ist keine der drei Hauptanomalien, sondern der Grund, warum Anomalien Ã¼berhaupt auftreten.",
            "difficulty": "einsteiger",
            "category": "Datenbank-Anomalien"
        },
        {
            "question": "Sie mÃ¶chten in der `KÃ¼nstler`-Tabelle den Interpreten 'Pink Floyd' lÃ¶schen. Obwohl es in der `CD`-Tabelle noch CDs dieser Band gibt, schlÃ¤gt die LÃ¶schung fehl. Was ist die wahrscheinlichste Ursache dafÃ¼r und wie kÃ¶nnen Sie das Problem lÃ¶sen, ohne Daten zu verlieren? ğŸš«",
            "options": [
                "Das liegt an einem Syntaxfehler im `DELETE`-Befehl.",
                "Die Datenbank erzwingt die referenzielle IntegritÃ¤t, um 'verwaiste' FremdschlÃ¼ssel in der `CD`-Tabelle zu verhindern. Sie mÃ¼ssen zuerst die zugehÃ¶rigen CDs lÃ¶schen oder ihre `Interpret_ID` auf NULL setzen.",
                "Der PrimÃ¤rschlÃ¼ssel in der `KÃ¼nstler`-Tabelle ist nicht eindeutig.",
                "Die Tabellen sind nicht korrekt in einer 1:n-Beziehung verknÃ¼pft."
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ›¡ï¸ Die referenzielle IntegritÃ¤t ist ein Mechanismus, der sicherstellt, dass FremdschlÃ¼ssel auf existierende PrimÃ¤rschlÃ¼ssel verweisen. Laut Handout kann ein Datensatz in der 'Eins'-Tabelle (`KÃ¼nstler`) nicht gelÃ¶scht werden, solange es noch zugehÃ¶rige DatensÃ¤tze in der 'Viele'-Tabelle (`CD`) gibt. Die Datenbank blockiert den LÃ¶schvorgang, um eine inkonsistente Datenlage zu verhindern. Die korrekte Vorgehensweise ist, entweder zuerst die abhÃ¤ngigen DatensÃ¤tze zu entfernen oder die FremdschlÃ¼ssel zu aktualisieren (z. B. auf NULL setzen), bevor der Datensatz in der Elterntabelle gelÃ¶scht wird.",
            "difficulty": "fortgeschritten",
            "category": "Referenzielle IntegritÃ¤t"
        },
        {
            "question": "Nehmen Sie an, die ursprÃ¼ngliche unnormalisierte Tabelle hat 5000 EintrÃ¤ge, wovon 200 EintrÃ¤ge fÃ¼r den `Interpret` einzigartig sind. Durch Normalisierung in 3NF wird die Tabelle in eine `KÃ¼nstler`-Tabelle (mit 200 EintrÃ¤gen) und eine `CD`-Tabelle (mit 5000 EintrÃ¤gen) aufgeteilt. Jede `KÃ¼nstler`-Zeile hat eine durchschnittliche Datenmenge von 50 Bytes, die in der unnormalisierten Tabelle redundant wÃ¤re. Wie viel Speicherplatz wird durch die Normalisierung ungefÃ¤hr eingespart? ğŸ’¾",
            "options": [
                "200 MB",
                "240 KB",
                "150 KB",
                "200 KB"
            ],
            "correct": 3,
            "explain": "**BegrÃ¼ndung:** In der unnormalisierten Tabelle wÃ¼rde die redundante Interpreten-Information bei jedem der 5000 EintrÃ¤ge gespeichert werden. ğŸ—ƒï¸ Da es nur 200 einzigartige Interpreten gibt, wird diese Information (50 Bytes pro Interpret) 5000 Mal statt nur 200 Mal gespeichert. Der zusÃ¤tzliche, redundante Speicherverbrauch wÃ¤re: $(5000 - 200) \times 50 \text{ Bytes} = 4800 \times 50 \text{ Bytes} = 240.000 \text{ Bytes}$. Das entspricht $240 \text{ KB}$. Durch die Normalisierung wird dieser redundante Speicherbedarf eliminiert. Die 50 Bytes pro Interpret werden in der neuen `KÃ¼nstler`-Tabelle nur einmal pro Interpret gespeichert. ğŸ”„ Die Einsparung ist also 240.000 Bytes oder 240 KB.",
            "difficulty": "fortgeschritten",
            "category": "Berechnungen"
        },
        {
            "question": "Wahr oder Falsch? ğŸ§ Ein Foreign Key Constraint mit `ON DELETE CASCADE` in der `CREATE TABLE`-Anweisung ermÃ¶glicht es, einen KÃ¼nstler aus der `KÃ¼nstler`-Tabelle zu lÃ¶schen, ohne dessen zugehÃ¶rige CDs zu entfernen, da der FremdschlÃ¼ssel dann automatisch auf NULL gesetzt wird.",
            "options": [
                "Wahr",
                "Falsch"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** âŒ Falsch. `ON DELETE CASCADE` bewirkt genau das Gegenteil: Wenn ein Datensatz in der Parent-Tabelle (`KÃ¼nstler`) gelÃ¶scht wird, werden alle abhÃ¤ngigen DatensÃ¤tze in der Child-Tabelle (`CD`) ebenfalls automatisch gelÃ¶scht. Dies stellt sicher, dass keine verwaisten DatensÃ¤tze zurÃ¼ckbleiben. ğŸ‘» Um den FremdschlÃ¼ssel auf NULL zu setzen, mÃ¼sste der Constraint `ON DELETE SET NULL` verwendet werden. Das Handout erwÃ¤hnt diesen wichtigen Unterschied.",
            "difficulty": "fortgeschritten",
            "category": "SQL und IntegritÃ¤t"
        },
        {
            "question": "Ein Datenbankentwickler analysiert eine bestehende Tabelle und stellt fest, dass das Attribut 'Albumtitel' funktional vom zusammengesetzten PrimÃ¤rschlÃ¼ssel (`Interpret`, `Albumtitel`) abhÃ¤ngig ist, jedoch nicht von `Interpret` allein. Welcher Normalisierungsgrad ist hier mindestens nicht erreicht? ğŸ¤”",
            "options": [
                "1. Normalform (1NF)",
                "2. Normalform (2NF)",
                "3. Normalform (3NF)",
                "4. Normalform (4NF)"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ“š Die 2. Normalform (2NF) verlangt, dass alle Nicht-SchlÃ¼ssel-Attribute vollstÃ¤ndig von der PrimÃ¤rschlÃ¼sselkandidat abhÃ¤ngen. Wenn der PrimÃ¤rschlÃ¼ssel zusammengesetzt ist (aus mehreren Spalten besteht), bedeutet dies, dass es keine partielle AbhÃ¤ngigkeit geben darf â€“ also kein Attribut darf nur von einem Teil des SchlÃ¼ssels abhÃ¤ngen. ğŸ”‘ Das beschriebene Szenario ist eine klassische partielle AbhÃ¤ngigkeit, die eine Tabelle daran hindert, die 2NF zu erreichen.",
            "difficulty": "fortgeschritten",
            "category": "Normalisierung"
        },
        {
            "question": "Eine Datenbank fÃ¼r ein Videospiel soll Spieler und ihre Errungenschaften speichern. Ein Spieler kann viele Errungenschaften haben, aber jede Errungenschaft gehÃ¶rt nur zu einem Spieler. Wie sollte diese 1:n-Beziehung im KrÃ¤henfuÃŸ-Modell (`Crow's Foot Notation`) korrekt dargestellt werden? ğŸ•¹ï¸",
            "options": [
                "Spieler â€”||â€” Errungenschaft",
                "Spieler â€”o<â€” Errungenschaft",
                "Spieler â€”o<|â€” Errungenschaft",
                "Spieler â€”||<â€” Errungenschaft"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ¦ Die KrÃ¤henfuÃŸ-Notation ist ein Standard fÃ¼r die Darstellung von KardinalitÃ¤ten. Der KrÃ¤henfuÃŸ (`<`) steht fÃ¼r 'viele', der senkrechte Strich (`|`) fÃ¼r 'eins'. Die Beziehung wird immer aus der Sicht der `many`-Tabelle gelesen. Da eine Errungenschaft zu genau einem Spieler gehÃ¶ren muss, ist die Beziehung von 'Errungenschaft' zu 'Spieler' 'exactly one', dargestellt durch `|`. Da ein Spieler beliebig viele Errungenschaften haben kann, ist die Beziehung von 'Spieler' zu 'Errungenschaft' 'one or many', dargestellt durch `|<`. Die Kombination ist also `--<`. Die richtige Option spiegelt diese Beziehung wider.",
            "difficulty": "einsteiger",
            "category": "Datenbankmodellierung"
        },
        {
            "question": "Welche der folgenden MaÃŸnahmen hilft, die Ã„nderungsanomalie in einer Datenbank zu verhindern? ğŸ”„",
            "options": [
                "EinfÃ¼hrung von `NULL`-Werten in SchlÃ¼sselfeldern.",
                "Entfernung von PrimÃ¤rschlÃ¼sseln, um Redundanz zu schaffen.",
                "Normalisierung der Datenbank, um redundante Daten zu minimieren.",
                "Verwendung von Transaktionen ohne `COMMIT`."
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ’¡ Die Ã„nderungsanomalie (Update Anomaly) entsteht durch Datenredundanz. Wenn dieselben Informationen an mehreren Stellen gespeichert sind, muss eine Ã„nderung an allen Stellen durchgefÃ¼hrt werden, um Inkonsistenzen zu vermeiden. Normalisierung eliminiert diese Redundanz, indem Daten nur an einer einzigen Stelle gespeichert werden. Dies stellt sicher, dass eine Ã„nderung nur einmal vorgenommen werden muss und die DatenintegritÃ¤t gewahrt bleibt. Die anderen Optionen wÃ¼rden das Problem verschlimmern oder sind nicht relevant.",
            "difficulty": "einsteiger",
            "category": "Datenbank-Anomalien"
        },
        {
            "question": "Ein Entwickler mÃ¶chte eine SQL-Anweisung schreiben, um einen neuen FremdschlÃ¼ssel-Constraint zur `CD`-Tabelle hinzuzufÃ¼gen, der auf die `KÃ¼nstler`-Tabelle verweist. Der Constraint soll die referenzielle IntegritÃ¤t sicherstellen, indem er verbietet, CDs hinzuzufÃ¼gen, wenn der Interpret nicht existiert. Welcher SQL-Befehl ist am besten geeignet, um diesen Constraint nachtrÃ¤glich in die Tabelle einzufÃ¼gen? ğŸ“",
            "options": [
                "CREATE TABLE cd ADD CONSTRAINT fk_kue FOREIGN KEY (interpret_id) REFERENCES kuenstler(interpret_id);",
                "UPDATE TABLE cd ADD CONSTRAINT fk_kue FOREIGN KEY (interpret_id) REFERENCES kuenstler(interpret_id);",
                "ALTER TABLE cd ADD CONSTRAINT fk_kue FOREIGN KEY (interpret_id) REFERENCES kuenstler(interpret_id);",
                "MODIFY TABLE cd ADD FOREIGN KEY (interpret_id) REFERENCES kuenstler(interpret_id);"
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ› ï¸ Der `ALTER TABLE`-Befehl wird verwendet, um die Struktur einer bereits existierenden Tabelle zu Ã¤ndern. In diesem Fall fÃ¼gen wir einen neuen Constraint (den FremdschlÃ¼ssel) hinzu. Die Syntax ist `ALTER TABLE [Tabellenname] ADD CONSTRAINT [Constraint-Name] FOREIGN KEY (...) REFERENCES ...`. `CREATE TABLE` wird nur fÃ¼r die Erstellung einer neuen Tabelle verwendet. `UPDATE` dient der Ã„nderung von DatensÃ¤tzen und `MODIFY` wird oft fÃ¼r die Ã„nderung von Spalteneigenschaften verwendet, nicht aber zum HinzufÃ¼gen von Constraints. ğŸ§‘â€ğŸ’»",
            "difficulty": "fortgeschritten",
            "category": "SQL und IntegritÃ¤t"
        },
        {
            "question": "Sie mÃ¼ssen eine Datenbank fÃ¼r eine groÃŸe E-Commerce-Plattform entwerfen. Sie haben die Wahl zwischen einem vollstÃ¤ndig normalisierten (3NF) und einem denormalisierten Schema. Analysieren Sie die Vor- und Nachteile beider AnsÃ¤tze in diesem Kontext. ğŸ›’",
            "options": [
                "3NF ist besser fÃ¼r Schreib-Operationen, da die Datenredundanz hoch ist, wÃ¤hrend Denormalisierung Lese-Operationen verlangsamt.",
                "Denormalisierung ist optimal fÃ¼r Transaktionen, da es die DatenintegritÃ¤t sicherstellt, wÃ¤hrend 3NF die Abfrageleistung verbessert.",
                "Ein 3NF-Schema ist optimal fÃ¼r Transaktionssysteme (OLTP) mit vielen SchreibvorgÃ¤ngen, da es DatenintegritÃ¤t und Konsistenz sicherstellt, kann aber komplexe Joins fÃ¼r Leseabfragen erfordern. Ein denormalisiertes Schema ist oft besser fÃ¼r analytische Systeme (OLAP), da es Abfragen beschleunigt, aber zu Datenredundanz und Anomalien fÃ¼hren kann.",
                "Es gibt keinen signifikanten Unterschied zwischen beiden AnsÃ¤tzen in puncto Leistung oder IntegritÃ¤t."
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ“ˆ Die Normalisierung in 3NF minimiert Redundanz und ist somit ideal fÃ¼r OLTP-Systeme (Online Transaction Processing) wie E-Commerce-Datenbanken, die viele `INSERT`, `UPDATE` und `DELETE`-Operationen verarbeiten. Sie stellt die Datenkonsistenz und IntegritÃ¤t sicher. Allerdings erfordern komplexere Abfragen (z.B. fÃ¼r Berichte) oft das ZusammenfÃ¼gen mehrerer Tabellen (`JOINs`), was die Leseleistung verlangsamen kann. Denormalisierung hingegen fÃ¼hrt Redundanz bewusst ein, um die Anzahl der Joins zu reduzieren und die Abfrageleistung (Read-Performance) zu verbessern. Daher wird sie hÃ¤ufig in Data-Warehousing- und Business-Intelligence-Systemen (OLAP) eingesetzt, wo Lesezugriffe dominieren. âš–ï¸",
            "difficulty": "experte",
            "category": "Vergleichende Analyse"
        },
        {
            "question": "Ein Datenbankadministrator stellt fest, dass die `Interpret_ID` im `CD`-Datensatz fÃ¼r 'Wish You Were Here' nicht mehr auf eine existierende `Interpret_ID` in der `KÃ¼nstler`-Tabelle verweist. Was ist die wahrscheinlichste Ursache fÃ¼r dieses Problem? ğŸšï¸",
            "options": [
                "Die Tabellen wurden falsch benannt.",
                "Es wurde eine `DELETE`-Operation in der `KÃ¼nstler`-Tabelle durchgefÃ¼hrt, ohne dass eine referenzielle IntegritÃ¤t erzwungen wurde.",
                "Der `cd_id` PrimÃ¤rschlÃ¼ssel ist nicht eindeutig.",
                "Die FremdschlÃ¼ssel wurden mit `ON UPDATE CASCADE` definiert."
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ•µï¸ Das Szenario beschreibt eine Verletzung der referenziellen IntegritÃ¤t. Laut Handout kÃ¶nnen solche 'verwaisten' FremdschlÃ¼ssel entstehen, wenn ein Datensatz in der 'Eins'-Tabelle (`KÃ¼nstler`) gelÃ¶scht wird, ohne dass das Datenbanksystem die Einhaltung der Referenziellen IntegritÃ¤t Ã¼berwacht. Eine solche LÃ¶schung wÃ¼rde in der `CD`-Tabelle einen FremdschlÃ¼ssel hinterlassen, der auf einen nicht mehr existierenden PrimÃ¤rschlÃ¼ssel verweist. ğŸ‘» Eine korrekte Datenbank mit einem Foreign Key Constraint wÃ¼rde diesen LÃ¶schversuch blockieren, es sei denn, es ist `ON DELETE CASCADE` gesetzt.",
            "difficulty": "fortgeschritten",
            "category": "Referenzielle IntegritÃ¤t"
        },
        {
            "question": "Was ist der Hauptunterschied zwischen der 1. Normalform (1NF) und der 2. Normalform (2NF)? âš–ï¸",
            "options": [
                "1NF verlangt, dass jede Spalte nur einen atomaren Wert enthÃ¤lt. 2NF verlangt zusÃ¤tzlich, dass alle Nicht-SchlÃ¼ssel-Attribute vollstÃ¤ndig vom PrimÃ¤rschlÃ¼ssel abhÃ¤ngen.",
                "1NF verlangt, dass alle Spalten eindeutige Namen haben. 2NF verlangt zusÃ¤tzlich, dass es keine transitiven AbhÃ¤ngigkeiten gibt.",
                "1NF verlangt, dass jede Tabelle einen PrimÃ¤rschlÃ¼ssel hat. 2NF verlangt zusÃ¤tzlich, dass keine Datenredundanz existiert.",
                "1NF erlaubt partielle AbhÃ¤ngigkeiten, wÃ¤hrend 2NF diese verbietet."
            ],
            "correct": 0,
            "explain": "**BegrÃ¼ndung:** ğŸ¤ Die 1. Normalform (1NF) ist die grundlegendste Regel und stellt sicher, dass jede Zelle einen einzigen (atomaren) Wert enthÃ¤lt. Die 2. Normalform (2NF) baut auf der 1NF auf und fÃ¼gt eine weitere Anforderung hinzu: Alle Nicht-SchlÃ¼ssel-Attribute mÃ¼ssen vom gesamten zusammengesetzten PrimÃ¤rschlÃ¼ssel abhÃ¤ngen. ğŸ“š Das bedeutet, es darf keine partielle AbhÃ¤ngigkeit geben, bei der ein Attribut nur von einem Teil des SchlÃ¼ssels abhÃ¤ngt. Die Aussage in Option D ist teilweise richtig, aber Option A ist die prÃ¤ziseste und umfassendste Definition.",
            "difficulty": "einsteiger",
            "category": "Normalisierung"
        },
        {
            "question": "Ein Entwickler hat zwei Tabellen, `KÃ¼nstler` und `CD`, erstellt. Nun mÃ¶chte er sicherstellen, dass das LÃ¶schen eines KÃ¼nstlers in der `KÃ¼nstler`-Tabelle automatisch alle zugehÃ¶rigen CDs in der `CD`-Tabelle lÃ¶scht. Wie muss der `FOREIGN KEY CONSTRAINT` in der `CREATE TABLE`-Anweisung der `CD`-Tabelle formuliert werden? ğŸ”¥",
            "options": [
                "REFERENCES kuenstler (interpret_id) ON DELETE RESTRICT",
                "REFERENCES kuenstler (interpret_id) ON DELETE SET NULL",
                "REFERENCES kuenstler (interpret_id) ON DELETE CASCADE",
                "REFERENCES kuenstler (interpret_id) ON DELETE NO ACTION"
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ§¨ Das SchlÃ¼sselwort `ON DELETE CASCADE` ist die korrekte Option, um ein solches Verhalten zu implementieren. Es weist das Datenbanksystem an, eine LÃ¶schaktion in der Parent-Tabelle (`KÃ¼nstler`) kaskadenartig fortzusetzen und alle abhÃ¤ngigen DatensÃ¤tze in der Child-Tabelle (`CD`) ebenfalls zu lÃ¶schen. `ON DELETE RESTRICT` (oder `NO ACTION`) wÃ¼rde die LÃ¶schung blockieren. `ON DELETE SET NULL` wÃ¼rde den FremdschlÃ¼sselwert auf NULL setzen, aber nicht den gesamten Datensatz lÃ¶schen. ğŸ’¥",
            "difficulty": "fortgeschritten",
            "category": "SQL und IntegritÃ¤t"
        },
        {
            "question": "Wahr oder Falsch? ğŸ“ Eine LÃ¶schanomalie tritt nur auf, wenn eine Tabelle einen PrimÃ¤rschlÃ¼ssel hat, der aus mehreren Attributen besteht.",
            "options": [
                "Wahr",
                "Falsch"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ™…â€â™‚ï¸ Falsch. Eine LÃ¶schanomalie (Deletion Anomaly) ist eine direkte Folge von Datenredundanz, nicht der Art des PrimÃ¤rschlÃ¼ssels. Sie tritt auf, wenn das LÃ¶schen eines Datensatzes unbeabsichtigt andere, nicht redundante Informationen mitlÃ¶scht. ğŸ—‘ï¸ Im Handout-Beispiel lÃ¶scht das Entfernen eines Albums auch die Bandinformation, obwohl die Bandinformation keine Teilmenge des PrimÃ¤rschlÃ¼ssels ist. Diese Anomalie kann in jeder unnormalisierten Tabelle auftreten, unabhÃ¤ngig davon, wie der PrimÃ¤rschlÃ¼ssel definiert ist.",
            "difficulty": "einsteiger",
            "category": "Datenbank-Anomalien"
        },
        {
            "question": "Angenommen, Sie haben eine nicht-normalisierte Tabelle mit den Spalten `(CD_ID, Albumtitel, Interpret, GrÃ¼ndungsjahr)`. Sie mÃ¶chten diese in die 3. Normalform (3NF) bringen. Welche der folgenden Tabellenstrukturen ist die korrekte ReprÃ¤sentation in 3NF? ğŸ“Š",
            "options": [
                "Tabelle A: `(CD_ID, Albumtitel, Interpret)` und Tabelle B: `(Interpret, GrÃ¼ndungsjahr)`",
                "Tabelle A: `(CD_ID, Albumtitel)` und Tabelle B: `(Albumtitel, Interpret, GrÃ¼ndungsjahr)`",
                "Tabelle A: `(CD_ID, Albumtitel, Interpret_ID)` und Tabelle B: `(Interpret_ID, Interpret, GrÃ¼ndungsjahr)`",
                "Tabelle A: `(CD_ID, Albumtitel)` und Tabelle B: `(Interpret, GrÃ¼ndungsjahr)`"
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ§‘â€ğŸ”¬ Die 3. Normalform (3NF) verlangt, dass es keine transitiven AbhÃ¤ngigkeiten gibt. In der ursprÃ¼nglichen Tabelle hÃ¤ngt `GrÃ¼ndungsjahr` von `Interpret` ab, nicht direkt von der `CD_ID`. Indem Sie `Interpret` und `GrÃ¼ndungsjahr` in eine separate `KÃ¼nstler`-Tabelle verschieben und die beiden Tabellen mit einem FremdschlÃ¼ssel (`Interpret_ID`) verknÃ¼pfen, eliminieren Sie die transitive AbhÃ¤ngigkeit. So wird die Redundanz (z. B. 'Pink Floyd' und '1965') entfernt und die 3NF erreicht. Die `CD_ID` bleibt der PrimÃ¤rschlÃ¼ssel der `CD`-Tabelle und der `Interpret_ID` ist der FremdschlÃ¼ssel, der auf den PrimÃ¤rschlÃ¼ssel der `KÃ¼nstler`-Tabelle verweist. ğŸ”—",
            "difficulty": "experte",
            "category": "Normalisierung"
        },
        {
            "question": "In einem komplexen Datenbankschema verwendet eine Tabelle einen zusammengesetzten PrimÃ¤rschlÃ¼ssel, z.B. aus den Spalten `(Kunden-ID, Bestell-ID)`. ErklÃ¤ren Sie, wie sich die Verwendung dieses SchlÃ¼ssels auf die Normalisierung auswirkt und welche Art von AbhÃ¤ngigkeit dabei besonders beachtet werden muss, um die 2. Normalform zu erreichen. ğŸ§©",
            "options": [
                "Die Normalisierung wird durch zusammengesetzte SchlÃ¼ssel nicht beeinflusst. Die 2. Normalform ist immer gewÃ¤hrleistet.",
                "Zusammengesetzte SchlÃ¼ssel kÃ¶nnen die Normalisierung erschweren, da sie das Risiko von partiellen AbhÃ¤ngigkeiten erhÃ¶hen, die es zu identifizieren und zu beseitigen gilt.",
                "Zusammengesetzte SchlÃ¼ssel sind in 2NF und 3NF nicht erlaubt.",
                "Ein zusammengesetzter SchlÃ¼ssel macht die 2. Normalform irrelevant, da diese nur fÃ¼r einfache PrimÃ¤rschlÃ¼ssel gilt."
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ§  Zusammengesetzte SchlÃ¼ssel sind entscheidend fÃ¼r die Normalisierung und erfordern eine genaue Betrachtung. Die 2. Normalform (2NF) befasst sich explizit mit partiellen AbhÃ¤ngigkeiten, die nur bei zusammengesetzten SchlÃ¼sseln auftreten kÃ¶nnen. â— Eine partielle AbhÃ¤ngigkeit liegt vor, wenn ein Nicht-SchlÃ¼ssel-Attribut nur von einem Teil des zusammengesetzten PrimÃ¤rschlÃ¼ssels abhÃ¤ngt. Ein Beispiel wÃ¤re, wenn das `Bestelldatum` nur von der `Bestell-ID` und nicht von der `Kunden-ID` abhÃ¤ngt. Um 2NF zu erreichen, mÃ¼ssen solche AbhÃ¤ngigkeiten in separate Tabellen Ã¼berfÃ¼hrt werden. So wird die DatenintegritÃ¤t sichergestellt und Redundanz vermieden.",
            "difficulty": "experte",
            "category": "Logische RÃ¤tsel"
        },
        {
            "question": "Was ist der Hauptzweck der referenziellen IntegritÃ¤t in einem relationalen Datenbanksystem? ğŸ¯",
            "options": [
                "Um sicherzustellen, dass jede Tabelle einen PrimÃ¤rschlÃ¼ssel hat.",
                "Um Datenredundanz zu eliminieren.",
                "Um zu verhindern, dass FremdschlÃ¼ssel auf nicht-existierende PrimÃ¤rschlÃ¼ssel verweisen.",
                "Um sicherzustellen, dass alle DatensÃ¤tze eindeutig sind."
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ‘® Referenzielle IntegritÃ¤t ist ein grundlegendes Konzept relationaler Datenbanken, das die Konsistenz der Beziehungen zwischen Tabellen gewÃ¤hrleistet. Sie stellt sicher, dass jeder Wert in einem FremdschlÃ¼ssel-Feld auch in der referenzierten PrimÃ¤rschlÃ¼ssel-Tabelle existiert. ğŸ”‘ Dies verhindert 'verwaiste' DatensÃ¤tze und schÃ¼tzt die logische VerknÃ¼pfung der Daten. Die anderen Optionen beschreiben entweder Merkmale von PrimÃ¤rschlÃ¼sseln oder sind das Ergebnis von Normalisierungsregeln, nicht aber der primÃ¤re Zweck der referenziellen IntegritÃ¤t.",
            "difficulty": "einsteiger",
            "category": "Referenzielle IntegritÃ¤t"
        },
        {
            "question": "Ein Datenbank-Design-Team diskutiert, ob es fÃ¼r die `CD`-Tabelle `ON DELETE CASCADE` oder `ON DELETE SET NULL` verwenden soll. Wann wÃ¤re `ON DELETE SET NULL` die bessere Wahl im Vergleich zu `ON DELETE CASCADE`? ğŸ’­",
            "options": [
                "Wenn Sie mÃ¶chten, dass das LÃ¶schen eines KÃ¼nstlers auch dessen CDs lÃ¶scht.",
                "Wenn Sie sicherstellen wollen, dass keine leeren FremdschlÃ¼ssel-Felder (`NULL`) in der `CD`-Tabelle vorhanden sind.",
                "Wenn Sie einen KÃ¼nstler aus der Datenbank entfernen mÃ¶chten, seine CDs aber weiterhin mit einer unbestimmten `Interpret_ID` (NULL) behalten wollen.",
                "Wenn die `KÃ¼nstler`-Tabelle keine PrimÃ¤rschlÃ¼ssel hat."
            ],
            "correct": 2,
            "explain": "**BegrÃ¼ndung:** ğŸ¤” `ON DELETE SET NULL` ist die bevorzugte Option, wenn die abhÃ¤ngigen Daten (z. B. die CDs) auch nach dem LÃ¶schen der Parent-EntitÃ¤t (z. B. des KÃ¼nstlers) logisch noch relevant sind und nicht mitgelÃ¶scht werden sollen. ğŸ—‘ï¸ Im Gegensatz zu `ON DELETE CASCADE`, das die CDs unwiderruflich lÃ¶schen wÃ¼rde, setzt `SET NULL` den FremdschlÃ¼sselwert auf NULL. Dies signalisiert, dass die CD existiert, aber nicht mehr einem bekannten KÃ¼nstler zugeordnet ist. Dies wÃ¤re z.B. sinnvoll, wenn eine Compilation-CD nach dem LÃ¶schen eines KÃ¼nstlers weiterhin in der Datenbank verbleiben soll. Beachten Sie, dass das FremdschlÃ¼sselfeld in diesem Fall NULL-Werte zulassen muss.",
            "difficulty": "experte",
            "category": "Vergleichende Analyse"
        },
        {
            "question": "Stellen Sie sich vor, in der unnormalisierten Tabelle soll der Albumtitel 'Not That Kind' von 'Not That Kind (mit der CD_ID)' auf 'Not That Kind (Anastacia)' geÃ¤ndert werden. Welche der drei Anomalien wÃ¼rde dies am ehesten betreffen, und warum ist eine vollstÃ¤ndige und konsistente Ã„nderung so herausfordernd? ğŸ¤¯",
            "options": [
                "Ã„nderungsanomalie: Da der Albumtitel redundant gespeichert ist, mÃ¼ssten potenziell mehrere DatensÃ¤tze aktualisiert werden. Ein Fehler bei der Aktualisierung kÃ¶nnte zu Inkonsistenzen fÃ¼hren.",
                "EinfÃ¼geanomalie: Das HinzufÃ¼gen des neuen Titels wÃ¤re schwierig, da der Datensatz bereits existiert.",
                "LÃ¶schanomalie: Der alte Albumtitel wÃ¼rde gelÃ¶scht, wodurch die Daten inkonsistent werden.",
                "Redundanzanomalie: Die Datenredundanz macht eine Ã„nderung unmÃ¶glich."
            ],
            "correct": 0,
            "explain": "**BegrÃ¼ndung:** âœï¸ Dies ist ein klassisches Beispiel fÃ¼r eine Ã„nderungsanomalie (Update Anomaly). Da der Albumtitel 'Not That Kind' in der unnormalisierten Tabelle an mehreren Stellen auftaucht, mÃ¼sste jede einzelne Instanz des Datensatzes manuell geÃ¤ndert werden. Vergisst man einen Datensatz oder macht einen Tippfehler, enthÃ¤lt die Datenbank anschlieÃŸend widersprÃ¼chliche oder inkonsistente Informationen. ğŸ¤¯ Normalisierung lÃ¶st dieses Problem, indem der Albumtitel nur an einer einzigen Stelle gespeichert wird.",
            "difficulty": "fortgeschritten",
            "category": "Szenario-basierte Fragen"
        },
        {
            "question": "Nehmen Sie an, Sie haben eine unnormalisierte Tabelle `(KundenID, Name, StraÃŸe, PLZ, Ort)`. Ein Ort kann mehrere Postleitzahlen haben. Wie wÃ¼rde sich eine `UPDATE`-Operation auf `PLZ` auswirken, um eine `Ã„nderungsanomalie` zu vermeiden, wenn ein Kunde in eine neue StraÃŸe mit einer anderen `PLZ` umzieht? ğŸšš",
            "options": [
                "Die `PLZ` muss nur einmal geÃ¤ndert werden, da die `StraÃŸe` nicht redundant ist.",
                "Da `PLZ` und `Ort` redundant sind, mÃ¼ssten nur diese beiden Felder in allen DatensÃ¤tzen des Kunden geÃ¤ndert werden.",
                "Da die `PLZ` nur in der Tabelle des Kunden geÃ¤ndert werden muss, gibt es keine `Ã„nderungsanomalie`.",
                "Die `PLZ` mÃ¼sste fÃ¼r alle Kunden, die am gleichen Ort wohnen, geÃ¤ndert werden, auch wenn sie nicht umziehen."
            ],
            "correct": 3,
            "explain": "**BegrÃ¼ndung:** ğŸŒ In diesem Szenario ist die `PLZ` transitiv vom `KundenID` Ã¼ber den `Ort` abhÃ¤ngig. Wenn ein Kunde umzieht und die `PLZ` geÃ¤ndert werden muss, betrifft dies nicht nur diesen einen Kunden, sondern alle Kunden am gleichen `Ort`, da die `PLZ` ein Attribut des `Orts` ist und nicht des `Kunden`. ğŸ˜ï¸ Um die DatenintegritÃ¤t zu wahren, mÃ¼sste die `PLZ` fÃ¼r alle Kunden des betreffenden Ortes geÃ¤ndert werden. Dies ist eine `Ã„nderungsanomalie`. Die LÃ¶sung wÃ¤re, eine separate Tabelle fÃ¼r `Orte` zu erstellen.",
            "difficulty": "experte",
            "category": "Logische RÃ¤tsel"
        },
        {
            "question": "Sie leiten eine Datenbank-Migration. Die alte Datenbank hat eine unnormalisierte Tabelle, die das `Erscheinungsjahr` fÃ¼r jedes Album einzeln speichert, auch wenn ein Album mehrmals in der Tabelle vorkommt. Das `Erscheinungsjahr` wird nun in der neuen Datenbank nur einmal pro Album gespeichert. SchÃ¤tzen Sie, wie viel Datenspeicher durch diese Normalisierung eingespart wird, wenn die alte Tabelle 10.000 EintrÃ¤ge hat, aber nur 2.000 einzigartige Alben, und das `Erscheinungsjahr` 4 Bytes belegt. ğŸ—„ï¸",
            "options": [
                "40 KB",
                "32 KB",
                "80 KB",
                "20 KB"
            ],
            "correct": 1,
            "explain": "**BegrÃ¼ndung:** ğŸ“‰ In der unnormalisierten Tabelle wird das `Erscheinungsjahr` 10.000 Mal gespeichert. Nach der Normalisierung nur noch 2.000 Mal. Die Redundanz betrifft $(10.000 - 2.000) = 8.000$ EintrÃ¤ge. Jedes `Erscheinungsjahr` belegt 4 Bytes. Die Einsparung betrÃ¤gt also $8.000 \times 4 \text{ Bytes} = 32.000 \text{ Bytes}$, was $32 \text{ KB}$ entspricht. ğŸ’¡ Die Berechnung zeigt den konkreten Nutzen der Normalisierung.",
            "difficulty": "fortgeschritten",
            "category": "Berechnungen"
        }
    ]
}